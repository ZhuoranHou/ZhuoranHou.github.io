[{"authors":["Zhuoran Hou"],"categories":null,"content":"I am currently a PhD student in Biostatistics from the department of Biostatistics and Bioinformatics at Duke University. Before moving to Duke, I have received extensive research training in statistical genetics, machine learning and epidemiological data analysis.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"eaf9b495430a5a533119bf30c6cb33e4","permalink":"https://ZhuoranHou.github.io/author/zhuoran-hou/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/zhuoran-hou/","section":"authors","summary":"I am currently a PhD student in Biostatistics from the department of Biostatistics and Bioinformatics at Duke University. Before moving to Duke, I have received extensive research training in statistical genetics, machine learning and epidemiological data analysis.","tags":null,"title":"Zhuoran Hou","type":"authors"},{"authors":["Ziyang Jiang","Zhuoran Hou","Yiling Liu","Yiman Ren","Keyu Li","David Carlson"],"categories":[],"content":"","date":1688169600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1688169600,"objectID":"8ea711414b0d9f95242cff3c2cf765ef","permalink":"https://ZhuoranHou.github.io/publication/jiang-2023-estimating/","publishdate":"2023-07-01T00:00:00Z","relpermalink":"/publication/jiang-2023-estimating/","section":"publication","summary":"A number of methods have been proposed for causal effect estimation, yet few have demonstrated efficacy in handling data with complex structures, such as images. To fill this gap, we propose Causal Multi-task Deep Ensemble (CMDE), a novel framework that learns both shared and group-specific information from the study population. We provide proofs demonstrating equivalency of CDME to a multi-task Gaussian process (GP) with a coregionalization kernel a priori. Compared to multi-task GP, CMDE efficiently handles high-dimensional and multi-modal covariates and provides pointwise uncertainty estimates of causal effects. We evaluate our method across various types of datasets and tasks and find that CMDE outperforms state-of-the-art methods on a majority of these tasks. ","tags":["Causal inference","Deep learning"],"title":"Estimating Causal Effects using a Multi-task Deep Ensemble","type":"publication"},{"authors":["Zhuoran Hou","Alejandro Ochoa"],"categories":[],"content":"","date":1677456000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1677456000,"objectID":"33ff0c3ed2c7b42cef0573e4b1822f31","permalink":"https://ZhuoranHou.github.io/publication/hou-2023-genetic/","publishdate":"2023-02-27T00:00:00Z","relpermalink":"/publication/hou-2023-genetic/","section":"publication","summary":"Common genetic association models for structured populations, including principal component analysis (PCA) and linear mixed-effects models (LMMs), model the correlation structure between individuals using population kinship matrices, also known as genetic relatedness matrices. However, the most common kinship estimators can have severe biases that were only recently determined. Here we characterize the effect of these kinship biases on genetic association. We employ a large simulated admixed family and genotypes from the 1000 Genomes Project, both with simulated traits, to evaluate key kinship estimators. Remarkably, we find practically invariant association statistics for kinship matrices of different bias types (matching all other features). We then prove using statistical theory and linear algebra that LMM association tests are invariant to these kinship biases, and PCA approximately so. Our proof shows that the intercept and relatedness effect coefficients compensate for the kinship bias, an argument that extends to generalized linear models. As a corollary, association testing is also invariant to changing the reference ancestral population of the kinship matrix. Lastly, we observed that all kinship estimators, except for popkin ratio-of-means, can give improper non-positive semidefinite matrices, which can be problematic although some LMMs handle them surprisingly well, and condition numbers can be used to choose kinship estimators. Overall, we find that existing association studies are robust to kinship estimation bias, and our calculations may help improve association methods by taking advantage of this unexpected robustness, as well as help determine the effects of kinship bias in related problems. ","tags":["Linear mixed model","Kinship matrices","population genetics"],"title":"Genetic association models are robust to common population kinship estimation biases","type":"publication"},{"authors":[],"categories":[],"content":"","date":1605574285,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1605574285,"objectID":"9cccaa4ac77a120d684469a88d8cce8f","permalink":"https://ZhuoranHou.github.io/project/covid-dashboard/","publishdate":"2020-11-16T19:51:25-05:00","relpermalink":"/project/covid-dashboard/","section":"project","summary":"Case level and individual level analysis","tags":[],"title":"Dashboard for COVID-19 in US","type":"project"},{"authors":[],"categories":[],"content":"The data project is a dashboard that provided updated COVID-19 information and presents model results.\nThe target audience of our dashboard is the general audience who want to know some COVID-19 information in US and what factors influence case growth rate. Thus, we added a lot of explanations in the dashboard using non-technical wording and omitted technical details of the model.\nProject goals We first discussed the goals of our project and decided to finalize it when we had the data. After the data collection, here are the goals of the project:\nBuild machine learning models to predict daily case growth rate\nExplore what factors influence case growth rate\nExplore the effect of Policy\nAnalyze what impact infection and death rate from individual case level\nBuild a dashboard that provided updated COVID19 information and presents model results\nData collection and data cleaning We mainly collected data from US CDC, a policy database supported by researchers in Boston University and US census bureau. Here are the main data resources we used in the project.\nCOVID-19 cases in US from January till now\nhttps://data.cdc.gov/Case-Surveillance/United-States-COVID-19-Cases-and-Deaths-by-State-o/9mfq-cb36\nCOVID-19 patient-level data\nhttps://data.cdc.gov/Case-Surveillance/COVID-19-Case-Surveillance-Public-Use-Data/vbim-akqf\nhospital resources (Hospital Capacity by State)\nhttps://www.cdc.gov/nhsn/covid19/report-patient-impact.html#anchor_1594393649\nCOVID-19 US State Policy\nhttps://github.com/USCOVIDpolicy/COVID-19-US-State-Policy-Database\nState demographic information\nhttps://www.census.gov/data/developers/data-sets/acs-1year.html\nWe download all the data through API from the website or just through the URL, and cleaned the data to make it at least meet the requirement of 1NF. Also, we created a codebook for each dataset.\nData analysis and Modeling There are three parts of our analysis:\n1.Comparison of the influence of policies on the increase rate of case among states\nIn this part, we try to explore the effectiveness of policies of different states on controlling COVID-19 cases. After discussion, we chose four main policy: stay at home order, face mask mandatory, restaurant close and business close. We calculated the average increase rate of total cases on each state 7/15 days before and after the implementation date of the each policy. 7 days represents short term while 15 days represents long term.\nWe also compare the state level increase rate and the increase rate of US to asses the performance of the state government.\n2.Predict Covid19 Case Growth Rate\nSince cases changes as time goes, we first considered LSTM (Long short-term memory) recurrent neural network (RNN) model and time series model, but the results were not good. Then, we decided only to consider increase rate and added several lags in the model to denote the effect of time instead of predicting new cases. After that, we used random forest models to train the data and made predictions.\nThere were two levels of analysis: US level and states level. For US level, we used hospital capacity data ( number of inpatient beds, number of ICU beds, etc.), state demographic information and policies to predict increase rate. For the state level, we only use hospital capacity data and policy data since state demographic information were constants for a specific state and would not change over time. We used data before July as training datasets and data after July as test datasets.\nThe test RMSE (root mean square error) of US level is 0.0107, which is quite low. We also trained a random forest model for each state.\n3.Individual Level Analysis\nSince the original dataset from US CDC has a large number of individual observations (over 5 million), we transformed the data into SQL database and performed stratified sampling based on the age group (5% of the data).\nThe treemap of the original data (we extracted 1.5M observations):\nThe treemap of the data after sampling:\nWe can see that we maintained the age structure by this sampling method. Then, we explored how age, sex and race influence the infection cases and fatality cases.\nData visualization and Development of data product We used streamlit to build a dashboard to show our data and analysis results.\nThere are four parts of the dashboard:\n1.Overview of the COVID-19 cases and death cases by states in US (updated everyday)\nThe trend of new cases and new death cases from January till now by state. Map plot showing total cases, new cases, cases per million people, death per million people based the latest data. 2.Comparison of the influence of policies on the increase rate of case among states\nTop 5 states with worse/best policy performance based on 7/15 day ranges Map plot showing the relative Increase Rate of individual states compare to the US 3.Predict case growth rate using hospital capacity, state demographic information and related policies based on Random forest.\nFeature Importance plot for the entire U.S. level Actual VS Predicted daily case growth rate for each state Feature Importance plot for the individual state level Pie chart showing state\u0026rsquo;s Sex ratio, race ratio Bar plot of age groups, data table of the state\u0026rsquo;s demographic information US level:\nState level:\n4.Explore how sex, age race and commodity affect the development of COVID-19 from a individual level\nSampling process of original data Tree plot of data specified by age_group Death cases count/rate for each age group; Infection cases count/rate for each age group (faceted by gender) Death cases count/rate for each race; Infection cases count/rate for each race (faceted by gender) Deployment We deployed the dashboard at this GitHub .Our final data product can be access from here.\n","date":1605574257,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1605574257,"objectID":"faba1b1f5a733065a1d0217797b60cdb","permalink":"https://ZhuoranHou.github.io/post/covid-dashboard/","publishdate":"2020-11-16T19:50:57-05:00","relpermalink":"/post/covid-dashboard/","section":"post","summary":"COVID-19 in US--Case level and individual level analysis","tags":["Python","streamlit","random forest"],"title":"Dashboard for COVID-19 in US","type":"post"},{"authors":[],"categories":[],"content":"How the neural network classified the images Image classification means that we use artificial intelligence (here we use deep learning) to automatically identify objects, people in images, and classify it to different labels.\nImages are composed of pixels, and these are the features of the images. Colors could be represented as RGB values (a combination of red, green and blue ranging from 0 to 255). When computer load images, it first convert images into arrays.\nNeural network (especially in deep learning) has multiple layers. The first layer usually takes in all the pixels within an image. And then, different filters are applied to the image.\nCredit: commons.wikimedia.org\nWe use a \u0026ldquo;convolutional layer\u0026rdquo; to extracting features from images, that where we have the commonly used term Convolutional Neural Network (CNN) in image classification.\nCredit: commons.wikimedia.org\nAfter we extract information from the convolutional layer, we will apply a pooling layer to the matrix. The main idea of \u0026ldquo;pooling\u0026rdquo; is to drop out redundant information and avoid overfitting.\nThe final layer before output is usually a dense layer or a fully connected layer, which is a linear operation on the layer’s input vector.\nCredit to:\rStanford CS class\nAn example based on insects images Train a deep learning model to classify beetles, cockroaches and dragonflies using these images(\rhttps://www.dropbox.com/s/fn73sj2e6c9rhf6/insects.zip?dl=0). Note: Original images from https://www.insectimages.org/index.cfm.\nExplore the dataset # Importing all necessary libraries import matplotlib.pyplot as plt\rimport numpy as np\rimport os\rimport PIL\rimport tensorflow as tf\rfrom tensorflow import keras\rfrom tensorflow.keras import layers\rfrom tensorflow.keras.models import Sequential\rimport pathlib\rdata_train_dir = pathlib.Path('insects\\\\train')\rdata_test_dir = pathlib.Path('insects\\\\test')\rdata_train_dir\rWindowsPath('C:/your_path/insects/train')\rimage_count = len(list(data_train_dir.glob('*/*.jpg')))\rprint(image_count)\r1019\rimage_count = len(list(data_test_dir.glob('*/*.jpg')))\rprint(image_count)\r180\rThere are 1019 figures for training and 180 figures for testing.\nLet\u0026rsquo;s take a look at the a sample figure.\ndragonflies = list(data_train_dir.glob('dragonflies/*'))\rPIL.Image.open(str(dragonflies[0]))\rLoad using keras.preprocessing img_width, img_height = 256, 224\rbatch_size = 32\rtrain_ds = tf.keras.preprocessing.image_dataset_from_directory(\rdata_train_dir,\rimage_size=(img_height, img_width),\rbatch_size=batch_size)\rFound 1019 files belonging to 3 classes.\rval_ds = tf.keras.preprocessing.image_dataset_from_directory(\rdata_test_dir,\rimage_size=(img_height, img_width),\rbatch_size=batch_size)\rFound 180 files belonging to 3 classes.\rclass_names = train_ds.class_names\rprint(class_names)\r['beetles', 'cockroach', 'dragonflies']\rfor image_batch, labels_batch in train_ds:\rprint(image_batch.shape)\rprint(labels_batch.shape)\rbreak\r(32, 224, 256, 3)\r(32,)\rThe image_batch is a tensor of the shape (32, 224, 256, 3). This is a batch of 32 images of shape 224x256x3\nStandardize the data normalization_layer = layers.experimental.preprocessing.Rescaling(1./255)\rnormalized_ds = train_ds.map(lambda x, y: (normalization_layer(x), y))\rimage_batch, labels_batch = next(iter(normalized_ds))\rfirst_image = image_batch[0]\r# Notice the pixels values are now in `[0,1]`.\rprint(np.min(first_image), np.max(first_image)) 0.0 0.9191305\rCreate the model num_classes = 3\rmodel = Sequential([\rlayers.experimental.preprocessing.Rescaling(1./255, input_shape=(img_height, img_width, 3)),\rlayers.Conv2D(16, 3, padding='same', activation='relu'),\rlayers.MaxPooling2D(),\rlayers.Conv2D(32, 3, padding='same', activation='relu'),\rlayers.MaxPooling2D(),\rlayers.Conv2D(64, 3, padding='same', activation='relu'),\rlayers.MaxPooling2D(),\rlayers.Flatten(),\rlayers.Dense(128, activation='relu'),\rlayers.Dense(num_classes)\r])\rI choose the optimizers.Adam optimizer and losses.SparseCategoricalCrossentropy loss function.\nmodel.compile(optimizer='adam',\rloss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\rmetrics=['accuracy'])\rmodel.summary()\rModel: \u0026quot;sequential\u0026quot;\r_________________________________________________________________\rLayer (type) Output Shape Param # =================================================================\rrescaling_1 (Rescaling) (None, 224, 256, 3) 0 _________________________________________________________________\rconv2d (Conv2D) (None, 224, 256, 16) 448 _________________________________________________________________\rmax_pooling2d (MaxPooling2D) (None, 112, 128, 16) 0 _________________________________________________________________\rconv2d_1 (Conv2D) (None, 112, 128, 32) 4640 _________________________________________________________________\rmax_pooling2d_1 (MaxPooling2 (None, 56, 64, 32) 0 _________________________________________________________________\rconv2d_2 (Conv2D) (None, 56, 64, 64) 18496 _________________________________________________________________\rmax_pooling2d_2 (MaxPooling2 (None, 28, 32, 64) 0 _________________________________________________________________\rflatten (Flatten) (None, 57344) 0 _________________________________________________________________\rdense (Dense) (None, 128) 7340160 _________________________________________________________________\rdense_1 (Dense) (None, 3) 387 =================================================================\rTotal params: 7,364,131\rTrainable params: 7,364,131\rNon-trainable params: 0\r_________________________________________________________________\rTrain the model epochs=10\rhistory = model.fit(\rtrain_ds,\rvalidation_data=val_ds,\repochs=epochs\r)\rEpoch 1/10\r32/32 [==============================] - 36s 1s/step - loss: 0.9313 - accuracy: 0.6340 - val_loss: 0.6570 - val_accuracy: 0.7667\rEpoch 2/10\r32/32 [==============================] - 37s 1s/step - loss: 0.4232 - accuracy: 0.8469 - val_loss: 0.5344 - val_accuracy: 0.8167\rEpoch 3/10\r32/32 [==============================] - 36s 1s/step - loss: 0.3108 - accuracy: 0.8803 - val_loss: 0.2941 - val_accuracy: 0.8944\rEpoch 4/10\r32/32 [==============================] - 35s 1s/step - loss: 0.1886 - accuracy: 0.9274 - val_loss: 0.1562 - val_accuracy: 0.9389\rEpoch 5/10\r32/32 [==============================] - 36s 1s/step - loss: 0.1031 - accuracy: 0.9617 - val_loss: 0.1012 - val_accuracy: 0.9611\rEpoch 6/10\r32/32 [==============================] - 36s 1s/step - loss: 0.0704 - accuracy: 0.9725 - val_loss: 0.0514 - val_accuracy: 0.9889\rEpoch 7/10\r32/32 [==============================] - 37s 1s/step - loss: 0.0435 - accuracy: 0.9902 - val_loss: 0.0399 - val_accuracy: 0.9944\rEpoch 8/10\r32/32 [==============================] - 38s 1s/step - loss: 0.0319 - accuracy: 0.9892 - val_loss: 0.1055 - val_accuracy: 0.9722\rEpoch 9/10\r32/32 [==============================] - 38s 1s/step - loss: 0.0284 - accuracy: 0.9921 - val_loss: 0.0279 - val_accuracy: 0.9944\rEpoch 10/10\r32/32 [==============================] - 39s 1s/step - loss: 0.0155 - accuracy: 0.9971 - val_loss: 0.0109 - val_accuracy: 0.9944\rVisualize training results acc = history.history['accuracy']\rval_acc = history.history['val_accuracy']\rloss = history.history['loss']\rval_loss = history.history['val_loss']\repochs_range = range(epochs)\rplt.figure(figsize=(8, 8))\rplt.subplot(1, 2, 1)\rplt.plot(epochs_range, acc, label='Training Accuracy')\rplt.plot(epochs_range, val_acc, label='Validation Accuracy')\rplt.legend(loc='lower right')\rplt.title('Training and Validation Accuracy')\rplt.subplot(1, 2, 2)\rplt.plot(epochs_range, loss, label='Training Loss')\rplt.plot(epochs_range, val_loss, label='Validation Loss')\rplt.legend(loc='upper right')\rplt.title('Training and Validation Loss')\rplt.show()\rThe model achieved quite high accuracy (99.44%).\nSave the model model.save_weights('model_saved.h5')\rReference 1.https://www.tensorflow.org/tutorials/images/classification 2.https://stackabuse.com/image-recognition-in-python-with-tensorflow-and-keras/ 3.https://cs231n.github.io/neural-networks-1/ 4.http://deeplearning.stanford.edu/tutorial/supervised/Pooling/\n","date":1604933584,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1604933584,"objectID":"39dbdcfad8b11aeb6fd7b85f9c13902b","permalink":"https://ZhuoranHou.github.io/post/deep_learning_for_insects_images/","publishdate":"2020-11-09T09:53:04-05:00","relpermalink":"/post/deep_learning_for_insects_images/","section":"post","summary":"How to apply Deep Learning for a Image classification task","tags":["Python","Deep learning","Tensorflow"],"title":"Deep learning for insects images classification","type":"post"},{"authors":[],"categories":[],"content":"","date":1604933542,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1604933542,"objectID":"98bf9dbf2e86d911c9906f6c4374fbcb","permalink":"https://ZhuoranHou.github.io/project/deep_learning_for_insects_images/","publishdate":"2020-11-09T09:52:22-05:00","relpermalink":"/project/deep_learning_for_insects_images/","section":"project","summary":"Use Tensorflow Keras to build an images classification model","tags":[],"title":"Deep learning for insects images classification","type":"project"},{"authors":[],"categories":[],"content":"","date":1601954767,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1601954767,"objectID":"f033ceefddaff6bf680d9eed86082d05","permalink":"https://ZhuoranHou.github.io/project/dashboard/","publishdate":"2020-10-05T23:26:07-04:00","relpermalink":"/project/dashboard/","section":"project","summary":"Use Dash package to create interactive dashboards","tags":[],"title":"Dashboard for PhDs dataset","type":"project"},{"authors":[],"categories":[],"content":"The task is to download data of PhDs awarded in the US. Do some analysis in pandas. Make a dashboard visualization of a few interesting aspects of the data using dash or streamlit. dataset: https://ncses.nsf.gov/pubs/nsf19301/data\n1. Read the dataset and data cleaning For this collection of datasets, I want to explore the number of doctorate recipients by major field of study, sex and citizenship in the selected years from 1987 to 2017. Thus, I planed to explore table 14 and table 17.\nimport pandas as pd\rimport numpy as np\r- Data cleaning for table 14 df1 = pd.read_excel('D:\\\\3.Duke\\\\course\\\\Year_2\\\\semester_1\\\\2_Bios823\\\\HW\\\\HW6\\\\data_tables\\\\sed17-sr-tab014.xlsx')\rChoose columnes that we are interested in. df1 = df1.iloc[7:,[0,1,3,5,7,9,11,13]] df1.columns = ['Field of study and sex','1987','1992','1997','2002','2007','2012','2017']\rdf1['Field of study and sex'][7] = 'Life sciences' ## rename some field names\rdf1['Field of study and sex'][28] = 'Other'\rdf1 = df1.reset_index().drop(columns=['index'])\rdf1['Field of study'] = df1.iloc[::3,0:1]['Field of study and sex'].repeat(repeats = 3).reset_index().drop(columns=['index'])\rdf1.head()\rField of study and sex\r1987\r1992\r1997\r2002\r2007\r2012\r2017\rField of study\r0\rLife sciences\r5783\r7141\r8365\r8465\r10694\r11949\r12587\rLife sciences\r1\rMale\r3747\r4322\r4613\r4443\r5197\r5335\r5629\rLife sciences\r2\rFemale\r2036\r2819\r3752\r4022\r5497\r6614\r6958\rLife sciences\r3\rPhysical sciences and earth sciences\r3811\r4486\r4522\r3871\r4955\r5414\r6079\rPhysical sciences and earth sciences\r4\rMale\r3177\r3545\r3489\r2812\r3447\r3684\r4068\rPhysical sciences and earth sciences\rMake the table tidy (into long form) df2 = pd.melt(df1, id_vars=['Field of study and sex','Field of study'],var_name='year', value_name='number')\rdf2.loc[~df2['Field of study and sex'].isin(['Male','Female']),'Field of study and sex'] = 'All'\rdf2 = df2.rename(columns={\u0026quot;Field of study and sex\u0026quot;: \u0026quot;sex\u0026quot;})\rdf2[\u0026quot;number\u0026quot;] = df2[\u0026quot;number\u0026quot;].astype(int)\rdf2[\u0026quot;year\u0026quot;] = df2[\u0026quot;year\u0026quot;].astype(int)\rdf2.head()\rsex\rField of study\ryear\rnumber\r0\rAll\rLife sciences\r1987\r5783\r1\rMale\rLife sciences\r1987\r3747\r2\rFemale\rLife sciences\r1987\r2036\r3\rAll\rPhysical sciences and earth sciences\r1987\r3811\r4\rMale\rPhysical sciences and earth sciences\r1987\r3177\rCreate a small subset that only contains overall information. df2a = df2[df2['sex'] == 'All'].reset_index().drop(columns=['index'])\rdf2a.head()\rsex\rField of study\ryear\rnumber\r0\rAll\rLife sciences\r1987\r5783\r1\rAll\rPhysical sciences and earth sciences\r1987\r3811\r2\rAll\rMathematics and computer sciences\r1987\r1189\r3\rAll\rPsychology and social sciences\r1987\r6063\r4\rAll\rEngineering\r1987\r3712\rCreate a small subset that contains information by sex. df2 = (\rdf2[df2['sex'].isin(['Male','Female'])].\rreset_index().drop(columns=['index'])\r)\rdf2.head()\rsex\rField of study\ryear\rnumber\r0\rMale\rLife sciences\r1987\r3747\r1\rFemale\rLife sciences\r1987\r2036\r2\rMale\rPhysical sciences and earth sciences\r1987\r3177\r3\rFemale\rPhysical sciences and earth sciences\r1987\r634\r4\rMale\rMathematics and computer sciences\r1987\r999\r- Data cleaning for table 17 The data cleaning for table 17 is similar to that of table 14.\ndf4 = pd.read_excel('D:\\\\3.Duke\\\\course\\\\Year_2\\\\semester_1\\\\2_Bios823\\\\HW\\\\HW6\\\\data_tables\\\\sed17-sr-tab017.xlsx')\rdf4 = pd.read_excel('D:\\\\3.Duke\\\\course\\\\Year_2\\\\semester_1\\\\2_Bios823\\\\HW\\\\HW6\\\\data_tables\\\\sed17-sr-tab017.xlsx')\rdf4 = df4.iloc[7:,:]\rdf4.columns = ['Field of study and citizenship','1987','1992','1997','2002','2007','2012','2017']\rdf4['Field of study and citizenship'][7] = 'Life sciences'\rdf4['Field of study and citizenship'][35] = 'Other'\rdf4 = df4.reset_index().drop(columns=['index'])\rdf4['Field of study'] = df4.iloc[::4,0:1]['Field of study and citizenship'].repeat(repeats = 4).reset_index().drop(columns=['index'])\rdf4['Field of study and citizenship'][7] = 'Life sciences'\rdf4['Field of study and citizenship'][35] = 'Other'\rdf5 = pd.melt(df4, id_vars=['Field of study and citizenship','Field of study'],var_name='year', value_name='number')\rdf5 = df5.rename(columns={\u0026quot;Field of study and citizenship\u0026quot;: \u0026quot;citizenship\u0026quot;})\rdf5[\u0026quot;number\u0026quot;] = df5[\u0026quot;number\u0026quot;].astype(int)\rdf5[\u0026quot;year\u0026quot;] = df5[\u0026quot;year\u0026quot;].astype(int)\rdf5 = (\rdf5[df5['citizenship'].isin(['U.S. citizen or permanent resident','Temporary visa holder','Unknown'])].\rreset_index().drop(columns=['index'])\r)\rdf5.head()\rcitizenship\rField of study\ryear\rnumber\r0\rU.S. citizen or permanent resident\rLife sciences\r1987\r4529\r1\rTemporary visa holder\rLife sciences\r1987\r939\r2\rUnknown\rLife sciences\r1987\r315\r3\rU.S. citizen or permanent resident\rPhysical sciences and earth sciences\r1987\r2657\r4\rTemporary visa holder\rPhysical sciences and earth sciences\r1987\r929\r2. Create dashboard visualizations Here I used the dash package to create dashboard in python.\nimport plotly.express as px\rimport jupyter_dash\rfrom jupyter_dash import JupyterDash\rimport dash_core_components as dcc\rimport dash_html_components as html\rfrom dash.dependencies import Input, Output\rDoctorate recipients by sex for selected years For this chart, you can drag the slider to change the selected year.\nexternal_stylesheets = ['https://codepen.io/chriddyp/pen/bWLwgP.css']\rapp = JupyterDash(__name__, external_stylesheets=external_stylesheets)\rapp.layout = html.Div([\rdcc.Graph(id='graph-with-slider'),\rdcc.Slider(\rid='year-slider',\rmin=df2['year'].min(),\rmax=df2['year'].max(),\rvalue=df2['year'].min(),\rmarks={str(year): str(year) for year in df2['year'].unique()},\rstep=None\r)\r])\r@app.callback(\rOutput('graph-with-slider', 'figure'),\r[Input('year-slider', 'value')])\rdef update_figure(selected_year):\rfiltered_df = df2[df2.year == selected_year]\rfig = px.bar(filtered_df, x=\u0026quot;Field of study\u0026quot;, y=\u0026quot;number\u0026quot;, color=\u0026quot;sex\u0026quot;, title=\u0026quot;Doctorate recipients by sex for selected years\u0026quot;)\rfig.update_layout(transition_duration=500)\rreturn fig\rapp.run_server(mode='inline',debug=True)\rThe Trend of number of doctorate recipients by sex from 1987 to 2017 For the chart below, you can play with the dropdown box and checklist to interact with the figure.\nexternal_stylesheets = ['https://codepen.io/chriddyp/pen/bWLwgP.css']\rapp = JupyterDash(__name__, external_stylesheets=external_stylesheets)\ravailable_fields = df2['Field of study'].unique()\rapp.layout = html.Div([\rhtml.Label('Field of study'),\rdcc.Dropdown(\rid='Field',\roptions=[{'label': i, 'value': i} for i in available_fields],\rvalue='Life sciences'\r),\rhtml.Label('Sex'), dcc.Checklist(\rid='sex',\roptions=[{'label': i, 'value': i} for i in ['Female', 'Male']],\rvalue=['Female', 'Male']\r) , dcc.Graph(id='fields-graphic'),\r])\r@app.callback(\rOutput('fields-graphic', 'figure'),\r[Input('Field', 'value'),\rInput('sex', 'value')\r])\rdef update_graph(field, sex):\rdff = df2[df2['Field of study'] == field]\rif sex == []:\rfig = px.line()\relif (sex == ['Female']) or ( sex == ['Male']) :\rfig = px.line(dff[dff['sex']== sex[0]], x=\u0026quot;year\u0026quot;, y=\u0026quot;number\u0026quot;)\relse:\rfig = px.line(dff, x=\u0026quot;year\u0026quot;, y=\u0026quot;number\u0026quot;, color='sex')\rreturn fig\rapp.run_server(mode='inline',debug=True)\rDoctorate recipients by major field of study and their according sex and citizenship information summary The charts below combine all the information in table 14 and table 17. When your mouse hover on the bar chart on the left, the sex and citizenship information in the according field will show on the right simultaneously.\nexternal_stylesheets = ['https://codepen.io/chriddyp/pen/bWLwgP.css']\rapp = JupyterDash(__name__, external_stylesheets=external_stylesheets)\ravailable_years = df2a['year'].unique()\rapp.layout = html.Div([\rhtml.Div([\rhtml.Div([\rdcc.Dropdown(\rid='year',\roptions=[{'label': i, 'value': i} for i in available_years],\rvalue='2017'\r),\r],\rstyle={'width': '49%', 'display': 'inline-block'}) ], style={\r'borderBottom': 'thin lightgrey solid',\r'backgroundColor': 'rgb(250, 250, 250)',\r'padding': '10px 5px'\r}), html.Div([\rdcc.Graph(\rid='allfield_barchart',\rhoverData={'points': [{'label': 'Life sciences'}]}\r)\r], style={ 'width': '49%', 'display': 'inline-block', 'padding': '0 20'}),\rhtml.Div([\rdcc.Graph(id='sex-prop'),\rdcc.Graph(id='citizen-prop'),\r], style={'display': 'inline-block', 'width': '49%'})\r])\r@app.callback(\rOutput('allfield_barchart', 'figure'),\r[Input('year', 'value')\r])\rdef update_graph(year_value):\rif year_value == None:\rfig = px.bar()\relse: dff = df2a[df2a['year'] == int(year_value)]\rfig = px.bar(dff, x=\u0026quot;Field of study\u0026quot;, y=\u0026quot;number\u0026quot;, template='seaborn',\rtitle=\u0026quot;Number of Doctorate recipients by field in %s\u0026quot; % year_value)\rfig.update_layout(autosize= True, height=800 ,\rmargin={'l': 20, 'b': 40, 't': 40, 'r': 20}, hovermode='closest')\rreturn fig\r@app.callback(\rOutput('sex-prop', 'figure'),\r[Input('allfield_barchart', 'hoverData'),\rInput('year', 'value')\r])\rdef update_sex_prop(hoverData, year_value):\rif year_value == None:\rfig = px.pie()\relse: field_name = hoverData['points'][0]['label']\rdff = df2[ (df2['year'] == int(year_value)) \u0026amp; (df2['Field of study'] == field_name ) ]\r#title = '\u0026lt;b\u0026gt;{}\u0026lt;/b\u0026gt;\u0026lt;br\u0026gt;{}'.format(country_name, xaxis_column_name)\rfig = px.pie(dff, values='number', names='sex',template='seaborn')\rfig.update_layout(legend=dict(orientation=\u0026quot;h\u0026quot;,\ryanchor=\u0026quot;bottom\u0026quot;,y=1.02,\rxanchor=\u0026quot;right\u0026quot;,x=1),\rtitle={'text': \u0026quot;sex\u0026quot;,'y':0.1, 'x':0.5})\rreturn fig\r@app.callback(\rOutput('citizen-prop', 'figure'),\r[Input('allfield_barchart', 'hoverData'),\rInput('year', 'value')\r])\rdef update_sex_prop(hoverData, year_value):\rif year_value == None:\rfig = px.pie()\relse: field_name = hoverData['points'][0]['label']\rdff = df5[ (df5['year'] == int(year_value)) \u0026amp; (df5['Field of study'] == field_name ) ]\r#title = '\u0026lt;b\u0026gt;{}\u0026lt;/b\u0026gt;\u0026lt;br\u0026gt;{}'.format(country_name, xaxis_column_name)\rfig = px.pie(dff, values='number', names='citizenship',template='plotly')\rfig.update_layout(legend=dict(orientation=\u0026quot;h\u0026quot;,\ryanchor=\u0026quot;bottom\u0026quot;,y=1.02,\rxanchor=\u0026quot;right\u0026quot;,x=1),\rtitle={'text': \u0026quot;citizenship\u0026quot;,'y':0.1, 'x':0.5})\rreturn fig\rapp.run_server(mode='inline',debug=True)\r","date":1601954750,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1601954750,"objectID":"994eece95a735fe6fcf13054af56d87a","permalink":"https://ZhuoranHou.github.io/post/dashboard/","publishdate":"2020-10-05T23:25:50-04:00","relpermalink":"/post/dashboard/","section":"post","summary":"Learn how to use Dash package to create interactive dashboards","tags":["Python","Pandas","Dash"],"title":"Dashboard for PhDs dataset","type":"post"},{"authors":[],"categories":[],"content":"","date":1601513891,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1601513891,"objectID":"c30f44109f62ff57e00b84222afd1d7c","permalink":"https://ZhuoranHou.github.io/project/starwar/","publishdate":"2020-09-30T20:58:11-04:00","relpermalink":"/project/starwar/","section":"project","summary":"Use requests and pandas packages to deal with the API","tags":[],"title":"Star Wars universe API data analysis","type":"project"},{"authors":[],"categories":[],"content":"The task is : use the requests library, download all the people in the Star Wars universe using the Star Wars API (\rhttps://swapi.dev/documentation). Show the name of the oldest person (or robot or alien) and list the titles of all the films they appeared in.\n1. Download Star Wars universe database import requests\rimport numpy as np\rimport pandas as pd\rimport os\rimport json\rbase_url = 'http://swapi.dev/api/'\ri = 1\rurls=[]\rwhile(True):\rurls.append(os.path.join(base_url, f'people/{i}'))\rif requests.get(os.path.join(base_url, f'people/{i+1}')).json() == {'detail': 'Not found'}:\ri +=1\rif requests.get(os.path.join(base_url, f'people/{i+1}')).json() == {'detail': 'Not found'}:\rbreak\ri +=1\rlen(urls)\r82\rpeople = [requests.get(url).json() for url in urls]\rpeople\r[{'name': 'Luke Skywalker',\r'height': '172',\r'mass': '77',\r'hair_color': 'blond',\r'skin_color': 'fair',\r'eye_color': 'blue',\r'birth_year': '19BBY',\r'gender': 'male',\r'homeworld': 'http://swapi.dev/api/planets/1/',\r'films': ['http://swapi.dev/api/films/1/',\r'http://swapi.dev/api/films/2/',\r'http://swapi.dev/api/films/3/',\r'http://swapi.dev/api/films/6/'],\r'species': [],\r'vehicles': ['http://swapi.dev/api/vehicles/14/',\r'http://swapi.dev/api/vehicles/30/'],\r'starships': ['http://swapi.dev/api/starships/12/',\r'http://swapi.dev/api/starships/22/'],\r'created': '2014-12-09T13:50:51.644000Z',\r'edited': '2014-12-20T21:17:56.891000Z',\r'url': 'http://swapi.dev/api/people/1/'},\r{'name': 'C-3PO',\r'height': '167',\r'mass': '75',\r'hair_color': 'n/a',\r'skin_color': 'gold',\r'eye_color': 'yellow',\r'birth_year': '112BBY',\r'gender': 'n/a',\r'homeworld': 'http://swapi.dev/api/planets/1/',\r'films': ['http://swapi.dev/api/films/1/',\r'http://swapi.dev/api/films/2/',\r'http://swapi.dev/api/films/3/',\r'http://swapi.dev/api/films/4/',\r'http://swapi.dev/api/films/5/',\r'http://swapi.dev/api/films/6/'],\r'species': ['http://swapi.dev/api/species/2/'],\r'vehicles': [],\r'starships': [],\r'created': '2014-12-10T15:10:51.357000Z',\r'edited': '2014-12-20T21:17:50.309000Z',\r'url': 'http://swapi.dev/api/people/2/'},\r............\r............\r............\r{'name': 'Tion Medon',\r'height': '206',\r'mass': '80',\r'hair_color': 'none',\r'skin_color': 'grey',\r'eye_color': 'black',\r'birth_year': 'unknown',\r'gender': 'male',\r'homeworld': 'http://swapi.dev/api/planets/12/',\r'films': ['http://swapi.dev/api/films/6/'],\r'species': ['http://swapi.dev/api/species/37/'],\r'vehicles': [],\r'starships': [],\r'created': '2014-12-20T20:35:04.260000Z',\r'edited': '2014-12-20T21:17:50.498000Z',\r'url': 'http://swapi.dev/api/people/83/'}]\rlen(people)\r82\rThen, I wrote a function to get nested film information.\ndef get_nested(d):\r\u0026quot;\u0026quot;\u0026quot;\rThis function is to download nested film information in the api.\rParameters\r----------\rd: dict\rthe json dict you downloaded\rReturns\r----------\rd: dict the nested json dict with additional film information\r\u0026quot;\u0026quot;\u0026quot;\rfilms_title=[]\rurls = d['films']\rfilms = [requests.get(url).json() for url in urls]\rfor x in films:\rfilms_title.append( {'title' : x['title']} ) d['films'] = films_title\rreturn d\rfor x in people:\rget_nested(x)\rpeople\r[{'name': 'Luke Skywalker',\r'height': '172',\r'mass': '77',\r'hair_color': 'blond',\r'skin_color': 'fair',\r'eye_color': 'blue',\r'birth_year': '19BBY',\r'gender': 'male',\r'homeworld': 'http://swapi.dev/api/planets/1/',\r'films': [{'title': 'A New Hope'},\r{'title': 'The Empire Strikes Back'},\r{'title': 'Return of the Jedi'},\r{'title': 'Revenge of the Sith'}],\r'species': [],\r'vehicles': ['http://swapi.dev/api/vehicles/14/',\r'http://swapi.dev/api/vehicles/30/'],\r'starships': ['http://swapi.dev/api/starships/12/',\r'http://swapi.dev/api/starships/22/'],\r'created': '2014-12-09T13:50:51.644000Z',\r'edited': '2014-12-20T21:17:56.891000Z',\r'url': 'http://swapi.dev/api/people/1/'},\r{'name': 'C-3PO',\r'height': '167',\r'mass': '75',\r'hair_color': 'n/a',\r'skin_color': 'gold',\r'eye_color': 'yellow',\r'birth_year': '112BBY',\r'gender': 'n/a',\r'homeworld': 'http://swapi.dev/api/planets/1/',\r'films': [{'title': 'A New Hope'},\r{'title': 'The Empire Strikes Back'},\r{'title': 'Return of the Jedi'},\r{'title': 'The Phantom Menace'},\r{'title': 'Attack of the Clones'},\r{'title': 'Revenge of the Sith'}],\r'species': ['http://swapi.dev/api/species/2/'],\r'vehicles': [],\r'starships': [],\r'created': '2014-12-10T15:10:51.357000Z',\r'edited': '2014-12-20T21:17:50.309000Z',\r'url': 'http://swapi.dev/api/people/2/'},\r............\r2. Data analysis df = pd.json_normalize(people)\rdf\rname\rheight\rmass\rhair_color\rskin_color\reye_color\rbirth_year\rgender\rhomeworld\rfilms\rspecies\rvehicles\rstarships\rcreated\redited\rurl\r0\rLuke Skywalker\r172\r77\rblond\rfair\rblue\r19BBY\rmale\rhttp://swapi.dev/api/planets/1/\r[{'title': 'A New Hope'}, {'title': 'The Empir...\r[]\r[http://swapi.dev/api/vehicles/14/, http://swa...\r[http://swapi.dev/api/starships/12/, http://sw...\r2014-12-09T13:50:51.644000Z\r2014-12-20T21:17:56.891000Z\rhttp://swapi.dev/api/people/1/\r1\rC-3PO\r167\r75\rn/a\rgold\ryellow\r112BBY\rn/a\rhttp://swapi.dev/api/planets/1/\r[{'title': 'A New Hope'}, {'title': 'The Empir...\r[http://swapi.dev/api/species/2/]\r[]\r[]\r2014-12-10T15:10:51.357000Z\r2014-12-20T21:17:50.309000Z\rhttp://swapi.dev/api/people/2/\r2\rR2-D2\r96\r32\rn/a\rwhite, blue\rred\r33BBY\rn/a\rhttp://swapi.dev/api/planets/8/\r[{'title': 'A New Hope'}, {'title': 'The Empir...\r[http://swapi.dev/api/species/2/]\r[]\r[]\r2014-12-10T15:11:50.376000Z\r2014-12-20T21:17:50.311000Z\rhttp://swapi.dev/api/people/3/\r3\rDarth Vader\r202\r136\rnone\rwhite\ryellow\r41.9BBY\rmale\rhttp://swapi.dev/api/planets/1/\r[{'title': 'A New Hope'}, {'title': 'The Empir...\r[]\r[]\r[http://swapi.dev/api/starships/13/]\r2014-12-10T15:18:20.704000Z\r2014-12-20T21:17:50.313000Z\rhttp://swapi.dev/api/people/4/\r4\rLeia Organa\r150\r49\rbrown\rlight\rbrown\r19BBY\rfemale\rhttp://swapi.dev/api/planets/2/\r[{'title': 'A New Hope'}, {'title': 'The Empir...\r[]\r[http://swapi.dev/api/vehicles/30/]\r[]\r2014-12-10T15:20:09.791000Z\r2014-12-20T21:17:50.315000Z\rhttp://swapi.dev/api/people/5/\r...\r...\r...\r...\r...\r...\r...\r...\r...\r...\r...\r...\r...\r...\r...\r...\r...\r77\rGrievous\r216\r159\rnone\rbrown, white\rgreen, yellow\runknown\rmale\rhttp://swapi.dev/api/planets/59/\r[{'title': 'Revenge of the Sith'}]\r[http://swapi.dev/api/species/36/]\r[http://swapi.dev/api/vehicles/60/]\r[http://swapi.dev/api/starships/74/]\r2014-12-20T19:43:53.348000Z\r2014-12-20T21:17:50.488000Z\rhttp://swapi.dev/api/people/79/\r78\rTarfful\r234\r136\rbrown\rbrown\rblue\runknown\rmale\rhttp://swapi.dev/api/planets/14/\r[{'title': 'Revenge of the Sith'}]\r[http://swapi.dev/api/species/3/]\r[]\r[]\r2014-12-20T19:46:34.209000Z\r2014-12-20T21:17:50.491000Z\rhttp://swapi.dev/api/people/80/\r79\rRaymus Antilles\r188\r79\rbrown\rlight\rbrown\runknown\rmale\rhttp://swapi.dev/api/planets/2/\r[{'title': 'A New Hope'}, {'title': 'Revenge o...\r[]\r[]\r[]\r2014-12-20T19:49:35.583000Z\r2014-12-20T21:17:50.493000Z\rhttp://swapi.dev/api/people/81/\r80\rSly Moore\r178\r48\rnone\rpale\rwhite\runknown\rfemale\rhttp://swapi.dev/api/planets/60/\r[{'title': 'Attack of the Clones'}, {'title': ...\r[]\r[]\r[]\r2014-12-20T20:18:37.619000Z\r2014-12-20T21:17:50.496000Z\rhttp://swapi.dev/api/people/82/\r81\rTion Medon\r206\r80\rnone\rgrey\rblack\runknown\rmale\rhttp://swapi.dev/api/planets/12/\r[{'title': 'Revenge of the Sith'}]\r[http://swapi.dev/api/species/37/]\r[]\r[]\r2014-12-20T20:35:04.260000Z\r2014-12-20T21:17:50.498000Z\rhttp://swapi.dev/api/people/83/\r82 rows × 16 columns\nFor the birth year of the person, using the in-universe standard of BBY or ABY - Before the Battle of Yavin or After the Battle of Yavin. The Battle of Yavin is a battle that occurs at the end of Star Wars episode IV: A New Hope.\nThus, we should only look at those with BBY.\ndf1 = df[df['birth_year'].str.contains(\u0026quot;BBY\u0026quot;)]\rdf1.head(2)\rname\rheight\rmass\rhair_color\rskin_color\reye_color\rbirth_year\rgender\rhomeworld\rfilms\rspecies\rvehicles\rstarships\rcreated\redited\rurl\r0\rLuke Skywalker\r172\r77\rblond\rfair\rblue\r19BBY\rmale\rhttp://swapi.dev/api/planets/1/\r[{'title': 'A New Hope'}, {'title': 'The Empir...\r[]\r[http://swapi.dev/api/vehicles/14/, http://swa...\r[http://swapi.dev/api/starships/12/, http://sw...\r2014-12-09T13:50:51.644000Z\r2014-12-20T21:17:56.891000Z\rhttp://swapi.dev/api/people/1/\r1\rC-3PO\r167\r75\rn/a\rgold\ryellow\r112BBY\rn/a\rhttp://swapi.dev/api/planets/1/\r[{'title': 'A New Hope'}, {'title': 'The Empir...\r[http://swapi.dev/api/species/2/]\r[]\r[]\r2014-12-10T15:10:51.357000Z\r2014-12-20T21:17:50.309000Z\rhttp://swapi.dev/api/people/2/\r​\n​\ndf1['birth_year'] = df1['birth_year'].str.rstrip('BBY')\rdf1['birth_year'] = pd.to_numeric(df1['birth_year'])\rdf1.head(2)\rname\rheight\rmass\rhair_color\rskin_color\reye_color\rbirth_year\rgender\rhomeworld\rfilms\rspecies\rvehicles\rstarships\rcreated\redited\rurl\r0\rLuke Skywalker\r172\r77\rblond\rfair\rblue\r19\rmale\rhttp://swapi.dev/api/planets/1/\r[{'title': 'A New Hope'}, {'title': 'The Empir...\r[]\r[http://swapi.dev/api/vehicles/14/, http://swa...\r[http://swapi.dev/api/starships/12/, http://sw...\r2014-12-09T13:50:51.644000Z\r2014-12-20T21:17:56.891000Z\rhttp://swapi.dev/api/people/1/\r1\rC-3PO\r167\r75\rn/a\rgold\ryellow\r112\rn/a\rhttp://swapi.dev/api/planets/1/\r[{'title': 'A New Hope'}, {'title': 'The Empir...\r[http://swapi.dev/api/species/2/]\r[]\r[]\r2014-12-10T15:10:51.357000Z\r2014-12-20T21:17:50.309000Z\rhttp://swapi.dev/api/people/2/\rdf1.loc[df1['birth_year'].idxmax()]['name']\r'Yoda'\rdf1.loc[df1['birth_year'].idxmax()]\rname Yoda\rheight 66\rmass 17\rhair_color white\rskin_color green\reye_color brown\rbirth_year 896\rgender male\rhomeworld http://swapi.dev/api/planets/28/\rfilms [{'title': 'The Empire Strikes Back'}, {'title...\rspecies [http://swapi.dev/api/species/6/]\rvehicles []\rstarships []\rcreated 2014-12-15T12:26:01.042000Z\redited 2014-12-20T21:17:50.345000Z\rurl http://swapi.dev/api/people/20/\rName: 18, dtype: object\rThus, the name of the oldest person (alien) is Yoda and list the titles of all the films they appeared in is:\ndf1.loc[df1['birth_year'].idxmax()]['films']\r[{'title': 'The Empire Strikes Back'},\r{'title': 'Return of the Jedi'},\r{'title': 'The Phantom Menace'},\r{'title': 'Attack of the Clones'},\r{'title': 'Revenge of the Sith'}]\r","date":1601513879,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1601513879,"objectID":"c4805f0aeb3496cc050da4964e4776c2","permalink":"https://ZhuoranHou.github.io/post/starwar/","publishdate":"2020-09-30T20:57:59-04:00","relpermalink":"/post/starwar/","section":"post","summary":"Use requests and pandas packages to deal with the API","tags":["Python","Pandas","API"],"title":"Star Wars universe API data analysis","type":"post"},{"authors":[],"categories":[],"content":"","date":1601474578,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1601474578,"objectID":"7649dfc75d7c0e142575611058310187","permalink":"https://ZhuoranHou.github.io/project/spotify/","publishdate":"2020-09-30T10:02:58-04:00","relpermalink":"/project/spotify/","section":"project","summary":"Create a Spotify dataset SQLite3 3NF schema","tags":[],"title":"Spotify dataset SQLite3 schema","type":"project"},{"authors":[],"categories":[],"content":"The task is to download the Spotify songs data set. Create a SQLite3 schema to store this data in at least 3rd normal form (3NF), and populate the tables. Use an SQL query to find the names of all playlists that contain instrumentals. Dataset: https://github.com/rfordatascience/tidytuesday/blob/master/data/2020/2020-01-21/readme.md\n1. Read the database import pandas as pd\rdf = pd.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-01-21/spotify_songs.csv')\rdf\rtrack_id\rtrack_name\rtrack_artist\rtrack_popularity\rtrack_album_id\rtrack_album_name\rtrack_album_release_date\rplaylist_name\rplaylist_id\rplaylist_genre\r...\rkey\rloudness\rmode\rspeechiness\racousticness\rinstrumentalness\rliveness\rvalence\rtempo\rduration_ms\r0\r6f807x0ima9a1j3VPbc7VN\rI Don't Care (with Justin Bieber) - Loud Luxur...\rEd Sheeran\r66\r2oCs0DGTsRO98Gh5ZSl2Cx\rI Don't Care (with Justin Bieber) [Loud Luxury...\r2019-06-14\rPop Remix\r37i9dQZF1DXcZDD7cfEKhW\rpop\r...\r6\r-2.634\r1\r0.0583\r0.102000\r0.000000\r0.0653\r0.5180\r122.036\r194754\r1\r0r7CVbZTWZgbTCYdfa2P31\rMemories - Dillon Francis Remix\rMaroon 5\r67\r63rPSO264uRjW1X5E6cWv6\rMemories (Dillon Francis Remix)\r2019-12-13\rPop Remix\r37i9dQZF1DXcZDD7cfEKhW\rpop\r...\r11\r-4.969\r1\r0.0373\r0.072400\r0.004210\r0.3570\r0.6930\r99.972\r162600\r2\r1z1Hg7Vb0AhHDiEmnDE79l\rAll the Time - Don Diablo Remix\rZara Larsson\r70\r1HoSmj2eLcsrR0vE9gThr4\rAll the Time (Don Diablo Remix)\r2019-07-05\rPop Remix\r37i9dQZF1DXcZDD7cfEKhW\rpop\r...\r1\r-3.432\r0\r0.0742\r0.079400\r0.000023\r0.1100\r0.6130\r124.008\r176616\r3\r75FpbthrwQmzHlBJLuGdC7\rCall You Mine - Keanu Silva Remix\rThe Chainsmokers\r60\r1nqYsOef1yKKuGOVchbsk6\rCall You Mine - The Remixes\r2019-07-19\rPop Remix\r37i9dQZF1DXcZDD7cfEKhW\rpop\r...\r7\r-3.778\r1\r0.1020\r0.028700\r0.000009\r0.2040\r0.2770\r121.956\r169093\r4\r1e8PAfcKUYoKkxPhrHqw4x\rSomeone You Loved - Future Humans Remix\rLewis Capaldi\r69\r7m7vv9wlQ4i0LFuJiE2zsQ\rSomeone You Loved (Future Humans Remix)\r2019-03-05\rPop Remix\r37i9dQZF1DXcZDD7cfEKhW\rpop\r...\r1\r-4.672\r1\r0.0359\r0.080300\r0.000000\r0.0833\r0.7250\r123.976\r189052\r...\r...\r...\r...\r...\r...\r...\r...\r...\r...\r...\r...\r...\r...\r...\r...\r...\r...\r...\r...\r...\r...\r32828\r7bxnKAamR3snQ1VGLuVfC1\rCity Of Lights - Official Radio Edit\rLush \u0026amp; Simon\r42\r2azRoBBWEEEYhqV6sb7JrT\rCity Of Lights (Vocal Mix)\r2014-04-28\r♥ EDM LOVE 2020\r6jI1gFr6ANFtT8MmTvA2Ux\redm\r...\r2\r-1.814\r1\r0.0936\r0.076600\r0.000000\r0.0668\r0.2100\r128.170\r204375\r32829\r5Aevni09Em4575077nkWHz\rCloser - Sultan \u0026amp; Ned Shepard Remix\rTegan and Sara\r20\r6kD6KLxj7s8eCE3ABvAyf5\rCloser Remixed\r2013-03-08\r♥ EDM LOVE 2020\r6jI1gFr6ANFtT8MmTvA2Ux\redm\r...\r0\r-4.462\r1\r0.0420\r0.001710\r0.004270\r0.3750\r0.4000\r128.041\r353120\r32830\r7ImMqPP3Q1yfUHvsdn7wEo\rSweet Surrender - Radio Edit\rStarkillers\r14\r0ltWNSY9JgxoIZO4VzuCa6\rSweet Surrender (Radio Edit)\r2014-04-21\r♥ EDM LOVE 2020\r6jI1gFr6ANFtT8MmTvA2Ux\redm\r...\r6\r-4.899\r0\r0.0481\r0.108000\r0.000001\r0.1500\r0.4360\r127.989\r210112\r32831\r2m69mhnfQ1Oq6lGtXuYhgX\rOnly For You - Maor Levi Remix\rMat Zo\r15\r1fGrOkHnHJcStl14zNx8Jy\rOnly For You (Remixes)\r2014-01-01\r♥ EDM LOVE 2020\r6jI1gFr6ANFtT8MmTvA2Ux\redm\r...\r2\r-3.361\r1\r0.1090\r0.007920\r0.127000\r0.3430\r0.3080\r128.008\r367432\r32832\r29zWqhca3zt5NsckZqDf6c\rTyphoon - Original Mix\rJulian Calor\r27\r0X3mUOm6MhxR7PzxG95rAo\rTyphoon/Storm\r2014-03-03\r♥ EDM LOVE 2020\r6jI1gFr6ANFtT8MmTvA2Ux\redm\r...\r5\r-4.571\r0\r0.0385\r0.000133\r0.341000\r0.7420\r0.0894\r127.984\r337500\r32833 rows × 23 columns\ndf.shape\r(32833, 23)\rdf.columns\rIndex(['track_id', 'track_name', 'track_artist', 'track_popularity',\r'track_album_id', 'track_album_name', 'track_album_release_date',\r'playlist_name', 'playlist_id', 'playlist_genre', 'playlist_subgenre',\r'danceability', 'energy', 'key', 'loudness', 'mode', 'speechiness',\r'acousticness', 'instrumentalness', 'liveness', 'valence', 'tempo',\r'duration_ms'],\rdtype='object')\r2. First Normal Form (1NF) Table has a primary key (unique, non-null column that identifies each row) No repeating groups of columns Each cell contains a single value It seems that the database is already 1NF.\n3. Second Normal Form (2NF) \u0026amp; Third Normal Form (3NF) Break partial dependencies Remove transitive dependencies df_songs = df.iloc[:, [0,1,2,3]].drop_duplicates()\rdf_songs\rtrack_id\rtrack_name\rtrack_artist\rtrack_popularity\r0\r6f807x0ima9a1j3VPbc7VN\rI Don't Care (with Justin Bieber) - Loud Luxur...\rEd Sheeran\r66\r1\r0r7CVbZTWZgbTCYdfa2P31\rMemories - Dillon Francis Remix\rMaroon 5\r67\r2\r1z1Hg7Vb0AhHDiEmnDE79l\rAll the Time - Don Diablo Remix\rZara Larsson\r70\r3\r75FpbthrwQmzHlBJLuGdC7\rCall You Mine - Keanu Silva Remix\rThe Chainsmokers\r60\r4\r1e8PAfcKUYoKkxPhrHqw4x\rSomeone You Loved - Future Humans Remix\rLewis Capaldi\r69\r...\r...\r...\r...\r...\r32828\r7bxnKAamR3snQ1VGLuVfC1\rCity Of Lights - Official Radio Edit\rLush \u0026amp; Simon\r42\r32829\r5Aevni09Em4575077nkWHz\rCloser - Sultan \u0026amp; Ned Shepard Remix\rTegan and Sara\r20\r32830\r7ImMqPP3Q1yfUHvsdn7wEo\rSweet Surrender - Radio Edit\rStarkillers\r14\r32831\r2m69mhnfQ1Oq6lGtXuYhgX\rOnly For You - Maor Levi Remix\rMat Zo\r15\r32832\r29zWqhca3zt5NsckZqDf6c\rTyphoon - Original Mix\rJulian Calor\r27\r28356 rows × 4 columns\ndf_albums = df.iloc[:, [4,5,6]].drop_duplicates()\rdf_albums\rtrack_album_id\rtrack_album_name\rtrack_album_release_date\r0\r2oCs0DGTsRO98Gh5ZSl2Cx\rI Don't Care (with Justin Bieber) [Loud Luxury...\r2019-06-14\r1\r63rPSO264uRjW1X5E6cWv6\rMemories (Dillon Francis Remix)\r2019-12-13\r2\r1HoSmj2eLcsrR0vE9gThr4\rAll the Time (Don Diablo Remix)\r2019-07-05\r3\r1nqYsOef1yKKuGOVchbsk6\rCall You Mine - The Remixes\r2019-07-19\r4\r7m7vv9wlQ4i0LFuJiE2zsQ\rSomeone You Loved (Future Humans Remix)\r2019-03-05\r...\r...\r...\r...\r32828\r2azRoBBWEEEYhqV6sb7JrT\rCity Of Lights (Vocal Mix)\r2014-04-28\r32829\r6kD6KLxj7s8eCE3ABvAyf5\rCloser Remixed\r2013-03-08\r32830\r0ltWNSY9JgxoIZO4VzuCa6\rSweet Surrender (Radio Edit)\r2014-04-21\r32831\r1fGrOkHnHJcStl14zNx8Jy\rOnly For You (Remixes)\r2014-01-01\r32832\r0X3mUOm6MhxR7PzxG95rAo\rTyphoon/Storm\r2014-03-03\r22545 rows × 3 columns\ndf_playlist = df.iloc[:, [7,8,9,10]].drop_duplicates()\rdf_playlist\rplaylist_name\rplaylist_id\rplaylist_genre\rplaylist_subgenre\r0\rPop Remix\r37i9dQZF1DXcZDD7cfEKhW\rpop\rdance pop\r70\rDance Pop\r37i9dQZF1DWZQaaqNMbbXa\rpop\rdance pop\r167\rDance Room\r37i9dQZF1DX2ENAPP1Tyed\rpop\rdance pop\r223\rCardio\r37i9dQZF1DWSJHnPb1f0X3\rpop\rdance pop\r272\rDance Pop Hits\r37i9dQZF1DX6pH08wMhkaI\rpop\rdance pop\r...\r...\r...\r...\r...\r32409\rFresh EDM | Progressive House | Electro House ...\r0FCHg9zJMNNiOokh3hVcxd\redm\rprogressive electro house\r32504\rFestival Music 2019 - Warm Up Music (EDM, Big ...\r73uj4YmsC7SJ6SbUMTvf07\redm\rprogressive electro house\r32582\rUnderground Party | Hypnotic | Minimal | Acid ...\r29jj7pQlDqnWclbHQk21Rq\redm\rprogressive electro house\r32674\rTrending EDM by Nik Cooper\r4N1ipiKR3xla8UXtE12XBm\redm\rprogressive electro house\r32753\r♥ EDM LOVE 2020\r6jI1gFr6ANFtT8MmTvA2Ux\redm\rprogressive electro house\r480 rows × 4 columns\ndf_attri = df.iloc[:, [0,11,12,13,14,15,16,17,18,19,20,21,22]].drop_duplicates()\rdf_attri\rtrack_id\rdanceability\renergy\rkey\rloudness\rmode\rspeechiness\racousticness\rinstrumentalness\rliveness\rvalence\rtempo\rduration_ms\r0\r6f807x0ima9a1j3VPbc7VN\r0.748\r0.916\r6\r-2.634\r1\r0.0583\r0.102000\r0.000000\r0.0653\r0.5180\r122.036\r194754\r1\r0r7CVbZTWZgbTCYdfa2P31\r0.726\r0.815\r11\r-4.969\r1\r0.0373\r0.072400\r0.004210\r0.3570\r0.6930\r99.972\r162600\r2\r1z1Hg7Vb0AhHDiEmnDE79l\r0.675\r0.931\r1\r-3.432\r0\r0.0742\r0.079400\r0.000023\r0.1100\r0.6130\r124.008\r176616\r3\r75FpbthrwQmzHlBJLuGdC7\r0.718\r0.930\r7\r-3.778\r1\r0.1020\r0.028700\r0.000009\r0.2040\r0.2770\r121.956\r169093\r4\r1e8PAfcKUYoKkxPhrHqw4x\r0.650\r0.833\r1\r-4.672\r1\r0.0359\r0.080300\r0.000000\r0.0833\r0.7250\r123.976\r189052\r...\r...\r...\r...\r...\r...\r...\r...\r...\r...\r...\r...\r...\r...\r32828\r7bxnKAamR3snQ1VGLuVfC1\r0.428\r0.922\r2\r-1.814\r1\r0.0936\r0.076600\r0.000000\r0.0668\r0.2100\r128.170\r204375\r32829\r5Aevni09Em4575077nkWHz\r0.522\r0.786\r0\r-4.462\r1\r0.0420\r0.001710\r0.004270\r0.3750\r0.4000\r128.041\r353120\r32830\r7ImMqPP3Q1yfUHvsdn7wEo\r0.529\r0.821\r6\r-4.899\r0\r0.0481\r0.108000\r0.000001\r0.1500\r0.4360\r127.989\r210112\r32831\r2m69mhnfQ1Oq6lGtXuYhgX\r0.626\r0.888\r2\r-3.361\r1\r0.1090\r0.007920\r0.127000\r0.3430\r0.3080\r128.008\r367432\r32832\r29zWqhca3zt5NsckZqDf6c\r0.603\r0.884\r5\r-4.571\r0\r0.0385\r0.000133\r0.341000\r0.7420\r0.0894\r127.984\r337500\r28356 rows × 13 columns\ndf_songs_albums = df.iloc[:, [0, 4]].drop_duplicates() ## link table\rdf_songs_albums\rtrack_id\rtrack_album_id\r0\r6f807x0ima9a1j3VPbc7VN\r2oCs0DGTsRO98Gh5ZSl2Cx\r1\r0r7CVbZTWZgbTCYdfa2P31\r63rPSO264uRjW1X5E6cWv6\r2\r1z1Hg7Vb0AhHDiEmnDE79l\r1HoSmj2eLcsrR0vE9gThr4\r3\r75FpbthrwQmzHlBJLuGdC7\r1nqYsOef1yKKuGOVchbsk6\r4\r1e8PAfcKUYoKkxPhrHqw4x\r7m7vv9wlQ4i0LFuJiE2zsQ\r...\r...\r...\r32828\r7bxnKAamR3snQ1VGLuVfC1\r2azRoBBWEEEYhqV6sb7JrT\r32829\r5Aevni09Em4575077nkWHz\r6kD6KLxj7s8eCE3ABvAyf5\r32830\r7ImMqPP3Q1yfUHvsdn7wEo\r0ltWNSY9JgxoIZO4VzuCa6\r32831\r2m69mhnfQ1Oq6lGtXuYhgX\r1fGrOkHnHJcStl14zNx8Jy\r32832\r29zWqhca3zt5NsckZqDf6c\r0X3mUOm6MhxR7PzxG95rAo\r28356 rows × 2 columns\ndf_songs_playlist = df.iloc[:, [0, 8]].drop_duplicates() ## link table\rdf_songs_playlist\rtrack_id\rplaylist_id\r0\r6f807x0ima9a1j3VPbc7VN\r37i9dQZF1DXcZDD7cfEKhW\r1\r0r7CVbZTWZgbTCYdfa2P31\r37i9dQZF1DXcZDD7cfEKhW\r2\r1z1Hg7Vb0AhHDiEmnDE79l\r37i9dQZF1DXcZDD7cfEKhW\r3\r75FpbthrwQmzHlBJLuGdC7\r37i9dQZF1DXcZDD7cfEKhW\r4\r1e8PAfcKUYoKkxPhrHqw4x\r37i9dQZF1DXcZDD7cfEKhW\r...\r...\r...\r32828\r7bxnKAamR3snQ1VGLuVfC1\r6jI1gFr6ANFtT8MmTvA2Ux\r32829\r5Aevni09Em4575077nkWHz\r6jI1gFr6ANFtT8MmTvA2Ux\r32830\r7ImMqPP3Q1yfUHvsdn7wEo\r6jI1gFr6ANFtT8MmTvA2Ux\r32831\r2m69mhnfQ1Oq6lGtXuYhgX\r6jI1gFr6ANFtT8MmTvA2Ux\r32832\r29zWqhca3zt5NsckZqDf6c\r6jI1gFr6ANFtT8MmTvA2Ux\r32251 rows × 2 columns\n4. Create a SQLite3 schema to store this data import sqlite3\rconn = sqlite3.connect('SpotifyDB.db') # You can create a new database by changing the name within the quotes\rc = conn.cursor() # The database will be saved in the location where your 'py' file is saved\rdf_songs.to_sql('songs', conn, if_exists='append', index = False) # Insert the values from the dataframe into the table 'songs' df_albums.to_sql('albums', conn, if_exists='append', index = False)\rdf_playlist.to_sql('playlist', conn, if_exists='append', index = False)\rdf_attri.to_sql('attri', conn, if_exists='append', index = False)\rdf_songs_albums.to_sql('songs_albums', conn, if_exists='append', index = False)\rdf_songs_playlist.to_sql('songs_playlist', conn, if_exists='append', index = False)\rc.close()\rconn.close()\r5.Use an SQL query to find the names of all playlists that contain instrumentals %load_ext sql\rThe sql extension is already loaded. To reload it, use:\r%reload_ext sql\r%sql sqlite:///SpotifyDB.db\r'Connected: @SpotifyDB.db'\r%%sql\rSELECT * FROM sqlite_master WHERE type='table';\rsqlite://\r* sqlite:///SpotifyDB.db\rDone.\rtype\rname\rtbl_name\rrootpage\rsql\rtable\rsongs\rsongs\r2\rCREATE TABLE \u0026quot;songs\u0026quot; (\n\u0026quot;track_id\u0026quot; TEXT,\n\u0026quot;track_name\u0026quot; TEXT,\n\u0026quot;track_artist\u0026quot; TEXT,\n\u0026quot;track_popularity\u0026quot; INTEGER\n)\rtable\ralbums\ralbums\r1288\rCREATE TABLE \u0026quot;albums\u0026quot; (\n\u0026quot;track_album_id\u0026quot; TEXT,\n\u0026quot;track_album_name\u0026quot; TEXT,\n\u0026quot;track_album_release_date\u0026quot; TEXT\n)\rtable\rplaylist\rplaylist\r2374\rCREATE TABLE \u0026quot;playlist\u0026quot; (\n\u0026quot;playlist_name\u0026quot; TEXT,\n\u0026quot;playlist_id\u0026quot; TEXT,\n\u0026quot;playlist_genre\u0026quot; TEXT,\n\u0026quot;playlist_subgenre\u0026quot; TEXT\n)\rtable\rattri\rattri\r2384\rCREATE TABLE \u0026quot;attri\u0026quot; (\n\u0026quot;track_id\u0026quot; TEXT,\n\u0026quot;danceability\u0026quot; REAL,\n\u0026quot;energy\u0026quot; REAL,\n\u0026quot;key\u0026quot; INTEGER,\n\u0026quot;loudness\u0026quot; REAL,\n\u0026quot;mode\u0026quot; INTEGER,\n\u0026quot;speechiness\u0026quot; REAL,\n\u0026quot;acousticness\u0026quot; REAL,\n\u0026quot;instrumentalness\u0026quot; REAL,\n\u0026quot;liveness\u0026quot; REAL,\n\u0026quot;valence\u0026quot; REAL,\n\u0026quot;tempo\u0026quot; REAL,\n\u0026quot;duration_ms\u0026quot; INTEGER\n)\rtable\rsongs_albums\rsongs_albums\r3191\rCREATE TABLE \u0026quot;songs_albums\u0026quot; (\n\u0026quot;track_id\u0026quot; TEXT,\n\u0026quot;track_album_id\u0026quot; TEXT\n)\rtable\rsongs_playlist\rsongs_playlist\r3558\rCREATE TABLE \u0026quot;songs_playlist\u0026quot; (\n\u0026quot;track_id\u0026quot; TEXT,\n\u0026quot;playlist_id\u0026quot; TEXT\n)\rAccording to the definition of instrumentalness:\nPredicts whether a track contains no vocals. “Ooh” and “aah” sounds are treated as instrumental in this context. Rap or spoken word tracks are clearly “vocal”. The closer the instrumentalness value is to 1.0, the greater likelihood the track contains no vocal content. Values above 0.5 are intended to represent instrumental tracks, but confidence is higher as the value approaches 1.0. Here, we use 0.5 as the threshold of if the song contains instrumentals\n%%sql Select DISTINCT playlist_name\rFrom songs\rInner join attri\rON songs.track_id = attri.track_id\rInner join songs_playlist\rON songs.track_id = songs_playlist.track_id\rInner join playlist\rON songs_playlist.playlist_id = playlist.playlist_id\rWhere instrumentalness \u0026gt; 0.5\rsqlite://\r* sqlite:///SpotifyDB.db\rDone.\rplaylist_name\rPop Remix\rDance Room\rPop Warmup 130 BPM\rDance Pop\rDance Pop Tunes\rPop / Dance\rMost Popular 2020 TOP 50\rpost-teen alternative, indie, pop (large variety)\rTodo Éxitos\rCharts 2020 🔥Top 2020🔥Hits 2020🔥Summer 2020🔥Pop 2020🔥Popular Music🔥Clean Pop 2020🔥Sing Alongs\r2020 Hits \u0026amp; 2019 Hits – Top Global Tracks 🔥🔥🔥\rPop Hits 2020\rMusic\u0026amp;Other Drugs\r90s Dance Hits\rCHRISTIAN ELECTRO / DANCE / EDM\rChristian Dance Party\r........\rEDM/POP\rSelected House\rVocal House\rElectro/Progressive/Club House\rUnderground Party | Hypnotic | Minimal | Acid | Big Room | Tech | Liquid\r","date":1601474557,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1601474557,"objectID":"7b24d28920cfbee1f647ee559da38322","permalink":"https://ZhuoranHou.github.io/post/spotify/","publishdate":"2020-09-30T10:02:37-04:00","relpermalink":"/post/spotify/","section":"post","summary":"Create a Spotify dataset SQLite3 3NF schema using pandas and sqlite3","tags":["Python","SQLlite3","SQL"],"title":"How to create a Spotify dataset SQLite3 schema","type":"post"},{"authors":[],"categories":[],"content":"","date":1601431792,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1601431792,"objectID":"e7ffdaab57ee8048b8343995811315fa","permalink":"https://ZhuoranHou.github.io/project/malaria_visual/","publishdate":"2020-09-29T22:09:52-04:00","relpermalink":"/project/malaria_visual/","section":"project","summary":"Visualizations of three Malaria Datasets","tags":[],"title":"Malaria Datasets Visualizations","type":"project"},{"authors":[],"categories":[],"content":"In this blog, I visualized three malaria datasets using Python. The datasets are from here: https://github.com/rfordatascience/tidytuesday/tree/master/data/2018/2018-11-13.\n%matplotlib inline\rimport matplotlib.pyplot as plt import pandas as pd\rimport seaborn as sns import numpy as np\rI first find a outside database about the continent information of countries, which I used in the following visualizations.\ndf0 = pd.read_csv(\u0026quot;https://datahub.io/JohnSnowLabs/country-and-continent-codes-list/r/country-and-continent-codes-list-csv.csv\u0026quot;)\rdf0\rContinent_Name\rContinent_Code\rCountry_Name\rTwo_Letter_Country_Code\rThree_Letter_Country_Code\rCountry_Number\r0\rAsia\rAS\rAfghanistan, Islamic Republic of\rAF\rAFG\r4.0\r1\rEurope\rEU\rAlbania, Republic of\rAL\rALB\r8.0\r2\rAntarctica\rAN\rAntarctica (the territory South of 60 deg S)\rAQ\rATA\r10.0\r3\rAfrica\rAF\rAlgeria, People's Democratic Republic of\rDZ\rDZA\r12.0\r4\rOceania\rOC\rAmerican Samoa\rAS\rASM\r16.0\r...\r...\r...\r...\r...\r...\r...\r257\rAfrica\rAF\rZambia, Republic of\rZM\rZMB\r894.0\r258\rOceania\rOC\rDisputed Territory\rXX\rNaN\rNaN\r259\rAsia\rAS\rIraq-Saudi Arabia Neutral Zone\rXE\rNaN\rNaN\r260\rAsia\rAS\rUnited Nations Neutral Zone\rXD\rNaN\rNaN\r261\rAsia\rAS\rSpratly Islands\rXS\rNaN\rNaN\r262 rows × 6 columns\ndfc = (\rdf0.iloc[:,[0,4]].\rdropna(subset=['Three_Letter_Country_Code']).\rdrop_duplicates(subset=['Three_Letter_Country_Code'])\r)\rdfc\rContinent_Name\rThree_Letter_Country_Code\r0\rAsia\rAFG\r1\rEurope\rALB\r2\rAntarctica\rATA\r3\rAfrica\rDZA\r4\rOceania\rASM\r...\r...\r...\r253\rSouth America\rVEN\r254\rOceania\rWLF\r255\rOceania\rWSM\r256\rAsia\rYEM\r257\rAfrica\rZMB\r250 rows × 2 columns\n1. Malaria deaths by country for all ages across the world and time. Read the dataset df1 = pd.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2018/2018-11-13/malaria_deaths.csv')\rdf1 = df1.dropna(subset=['Code']) ## drop those are not countries\rdf1 = df1[df1['Entity'] != 'World'] ## delete whole world data\rdf1\rEntity\rCode\rYear\rDeaths - Malaria - Sex: Both - Age: Age-standardized (Rate) (per 100,000 people)\r0\rAfghanistan\rAFG\r1990\r6.802930\r1\rAfghanistan\rAFG\r1991\r6.973494\r2\rAfghanistan\rAFG\r1992\r6.989882\r3\rAfghanistan\rAFG\r1993\r7.088983\r4\rAfghanistan\rAFG\r1994\r7.392472\r...\r...\r...\r...\r...\r6151\rZimbabwe\rZWE\r2012\r9.751727\r6152\rZimbabwe\rZWE\r2013\r9.419939\r6153\rZimbabwe\rZWE\r2014\r9.487453\r6154\rZimbabwe\rZWE\r2015\r9.440046\r6155\rZimbabwe\rZWE\r2016\r9.507309\r5265 rows × 4 columns\nRename the column names df1.columns=['Entity', 'Code', 'Year','Mortality rate']\rChoose top ten countries with the highest mean mortality rate from 1990 to 2016 df1_m = ( df1.groupby(by=\u0026quot;Entity\u0026quot;).agg('mean').\rsort_values(by='Mortality rate',ascending=False).\riloc[:10,1:].copy()\r)\rdf1_m = df1_m.reset_index().copy()\rdf1_m\rEntity\rMortality rate\r0\rSierra Leone\r181.658950\r1\rBurkina Faso\r169.667275\r2\rUganda\r147.281621\r3\rEquatorial Guinea\r142.713265\r4\rCote d'Ivoire\r140.610546\r5\rNigeria\r135.902567\r6\rNiger\r135.766525\r7\rDemocratic Republic of Congo\r134.571369\r8\rBurundi\r131.296440\r9\rMali\r122.955185\rdf1_m['Entity'][7] = \u0026quot;DR Congo\u0026quot;\rdf1_m\rEntity\rMortality rate\r0\rSierra Leone\r181.658950\r1\rBurkina Faso\r169.667275\r2\rUganda\r147.281621\r3\rEquatorial Guinea\r142.713265\r4\rCote d'Ivoire\r140.610546\r5\rNigeria\r135.902567\r6\rNiger\r135.766525\r7\rDR Congo\r134.571369\r8\rBurundi\r131.296440\r9\rMali\r122.955185\rPlot the bar plot fig, axes = plt.subplots(1, 1, figsize=(10,4))\rg1 = sns.barplot(x='Entity', y='Mortality rate', data=df1_m)\rlabels = df1_m['Entity']\rg1.set_xticklabels(labels,rotation=-45)\rplt.title('Top ten countries with the highest mean mortality rate from 1990 to 2016')\rpass\rPlot the trend of mortality rate of these ten countries df1_ss = df1[df1['Entity'].isin(df1_m['Entity'])].copy()\rdf1_ss\rEntity\rCode\rYear\rMortality rate\r810\rBurkina Faso\rBFA\r1990\r148.890249\r811\rBurkina Faso\rBFA\r1991\r154.470849\r812\rBurkina Faso\rBFA\r1992\r158.051907\r813\rBurkina Faso\rBFA\r1993\r162.043022\r814\rBurkina Faso\rBFA\r1994\r164.068758\r...\r...\r...\r...\r...\r5692\rUganda\rUGA\r2012\r69.818389\r5693\rUganda\rUGA\r2013\r54.161004\r5694\rUganda\rUGA\r2014\r48.320006\r5695\rUganda\rUGA\r2015\r50.672187\r5696\rUganda\rUGA\r2016\r49.232941\r243 rows × 4 columns\nfig, axes = plt.subplots(1, 1, figsize=(15,8))\rsns.lineplot(x='Year', y='Mortality rate',hue = 'Entity' , data=df1_ss)\rplt.title('Mortality rate for top ten countries with the highest mortality rate from 1990 to 2016')\rplt.legend(loc='best', fontsize=10)\rpass\r2. Malaria deaths by age across the world and time. Read the dataset df2 = pd.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2018/2018-11-13/malaria_deaths_age.csv',index_col = 0)\rdf2\rentity\rcode\ryear\rage_group\rdeaths\r1\rAfghanistan\rAFG\r1990\rUnder 5\r184.606435\r2\rAfghanistan\rAFG\r1991\rUnder 5\r191.658193\r3\rAfghanistan\rAFG\r1992\rUnder 5\r197.140197\r4\rAfghanistan\rAFG\r1993\rUnder 5\r207.357753\r5\rAfghanistan\rAFG\r1994\rUnder 5\r226.209363\r...\r...\r...\r...\r...\r...\r30776\rZimbabwe\rZWE\r2012\r50-69\r103.185111\r30777\rZimbabwe\rZWE\r2013\r50-69\r100.113293\r30778\rZimbabwe\rZWE\r2014\r50-69\r99.013890\r30779\rZimbabwe\rZWE\r2015\r50-69\r98.091738\r30780\rZimbabwe\rZWE\r2016\r50-69\r97.402058\r30780 rows × 5 columns\ndf2 = df2.dropna(subset=['code']) ## drop those are not countries\rdf2 = df2[df2['entity'] != 'World'] ## delete whole world data\rdf2\rentity\rcode\ryear\rage_group\rdeaths\r1\rAfghanistan\rAFG\r1990\rUnder 5\r184.606435\r2\rAfghanistan\rAFG\r1991\rUnder 5\r191.658193\r3\rAfghanistan\rAFG\r1992\rUnder 5\r197.140197\r4\rAfghanistan\rAFG\r1993\rUnder 5\r207.357753\r5\rAfghanistan\rAFG\r1994\rUnder 5\r226.209363\r...\r...\r...\r...\r...\r...\r30776\rZimbabwe\rZWE\r2012\r50-69\r103.185111\r30777\rZimbabwe\rZWE\r2013\r50-69\r100.113293\r30778\rZimbabwe\rZWE\r2014\r50-69\r99.013890\r30779\rZimbabwe\rZWE\r2015\r50-69\r98.091738\r30780\rZimbabwe\rZWE\r2016\r50-69\r97.402058\r26325 rows × 5 columns\nchoose top ten countries with most death cases df2_t2 = df2[df2['entity'].isin(df2_t['entity'])].copy()\rdf2_t2\rentity\rcode\ryear\rage_group\rdeaths\r811\rBurkina Faso\rBFA\r1990\rUnder 5\r18239.856090\r812\rBurkina Faso\rBFA\r1991\rUnder 5\r19262.190962\r813\rBurkina Faso\rBFA\r1992\rUnder 5\r20171.050076\r814\rBurkina Faso\rBFA\r1993\rUnder 5\r21023.473688\r815\rBurkina Faso\rBFA\r1994\rUnder 5\r21625.799178\r...\r...\r...\r...\r...\r...\r30317\rUganda\rUGA\r2012\r50-69\r1223.126262\r30318\rUganda\rUGA\r2013\r50-69\r1000.050301\r30319\rUganda\rUGA\r2014\r50-69\r949.020874\r30320\rUganda\rUGA\r2015\r50-69\r1083.887084\r30321\rUganda\rUGA\r2016\r50-69\r1086.361351\r1350 rows × 5 columns\ndf2_t2.loc[df2_t2['entity'] == 'Democratic Republic of Congo','entity'] = 'DR Congo'\rdf2_t3 = df2_t2.groupby(by=[\u0026quot;entity\u0026quot;,\u0026quot;age_group\u0026quot;]).agg('mean').iloc[:,[1]].unstack('age_group')\rdf2_t3.columns = ['15-49','5-14','50-69','70 or older','Under 5']\rdf2_t3\r15-49\r5-14\r50-69\r70 or older\rUnder 5\rentity\rBurkina Faso\r1440.986219\r2180.110659\r949.531799\r616.307888\r27090.929143\rCameroon\r1841.799877\r2087.751841\r1260.056940\r843.155975\r18920.406820\rCote d'Ivoire\r2435.445658\r2707.015515\r1540.863000\r961.537118\r19893.376484\rDR Congo\r5865.025596\r7418.616598\r3738.726743\r2173.788505\r78082.393657\rIndia\r7467.822952\r12800.418534\r4570.759705\r2180.711296\r33569.693452\rMozambique\r1768.206804\r2589.157880\r942.746073\r646.568790\r24264.110234\rNiger\r1049.787630\r1951.445501\r754.023705\r476.335101\r21820.324927\rNigeria\r9019.184357\r17961.554380\r6505.248814\r4558.828812\r213128.814854\rTanzania\r2282.382476\r2561.401544\r1270.799946\r814.388226\r24197.304098\rUganda\r3213.540577\r4041.199036\r1844.906441\r1290.584647\r40908.586984\rdf2_t3.plot(kind='bar', stacked=True,figsize=(15, 10),rot = -45, title = \u0026quot;Malaria Year Deaths means by age group from top ten countries with most death cases \u0026quot;)\rpass\rChoose Nigeria to see the trend of death cases across countries df2_nig = df2[df2['entity'] == 'Nigeria' ].copy()\rdf2_nig\rentity\rcode\ryear\rage_group\rdeaths\r3889\rNigeria\rNGA\r1990\rUnder 5\r169612.233561\r3890\rNigeria\rNGA\r1991\rUnder 5\r177084.049314\r3891\rNigeria\rNGA\r1992\rUnder 5\r182532.593757\r3892\rNigeria\rNGA\r1993\rUnder 5\r189569.206430\r3893\rNigeria\rNGA\r1994\rUnder 5\r194656.088044\r...\r...\r...\r...\r...\r...\r28535\rNigeria\rNGA\r2012\r50-69\r6362.466362\r28536\rNigeria\rNGA\r2013\r50-69\r6225.805881\r28537\rNigeria\rNGA\r2014\r50-69\r6309.529067\r28538\rNigeria\rNGA\r2015\r50-69\r6436.740885\r28539\rNigeria\rNGA\r2016\r50-69\r6489.088313\r135 rows × 5 columns\nfig, axes = plt.subplots(1, 1, figsize=(15,8))\rsns.lineplot(x='year', y='deaths',hue = 'age_group', hue_order = ['Under 5','5-14','15-49','50-69','70 or older'] , data=df2_nig)\rplt.title('Malaria Deaths Rate in Nigeria by age group from 1990 to 2016')\rplt.legend(loc='best', fontsize=10)\rpass\r3. Malaria incidence by country for all ages across the world across time Read dataset df3 = pd.read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2018/2018-11-13/malaria_inc.csv')\rdf3\rEntity\rCode\rYear\rIncidence of malaria (per 1,000 population at risk) (per 1,000 population at risk)\r0\rAfghanistan\rAFG\r2000\r107.100000\r1\rAfghanistan\rAFG\r2005\r46.500000\r2\rAfghanistan\rAFG\r2010\r23.900000\r3\rAfghanistan\rAFG\r2015\r23.600000\r4\rAlgeria\rDZA\r2000\r0.037746\r...\r...\r...\r...\r...\r503\rZambia\rZMB\r2015\r173.700000\r504\rZimbabwe\rZWE\r2000\r143.200000\r505\rZimbabwe\rZWE\r2005\r142.500000\r506\rZimbabwe\rZWE\r2010\r129.600000\r507\rZimbabwe\rZWE\r2015\r114.200000\r508 rows × 4 columns\ndf3 = df3.dropna(subset=['Code']) ## drop those are not countries\rdf3 = df3[df3['Entity'] != 'World'] ## delete whole world data\rdf3\rEntity\rCode\rYear\rIncidence of malaria (per 1,000 population at risk) (per 1,000 population at risk)\r0\rAfghanistan\rAFG\r2000\r107.100000\r1\rAfghanistan\rAFG\r2005\r46.500000\r2\rAfghanistan\rAFG\r2010\r23.900000\r3\rAfghanistan\rAFG\r2015\r23.600000\r4\rAlgeria\rDZA\r2000\r0.037746\r...\r...\r...\r...\r...\r503\rZambia\rZMB\r2015\r173.700000\r504\rZimbabwe\rZWE\r2000\r143.200000\r505\rZimbabwe\rZWE\r2005\r142.500000\r506\rZimbabwe\rZWE\r2010\r129.600000\r507\rZimbabwe\rZWE\r2015\r114.200000\r396 rows × 4 columns\ndf3.columns=['Entity', 'Code', 'Year', 'Incidence of malaria']\rdf3\rEntity\rCode\rYear\rIncidence of malaria\r0\rAfghanistan\rAFG\r2000\r107.100000\r1\rAfghanistan\rAFG\r2005\r46.500000\r2\rAfghanistan\rAFG\r2010\r23.900000\r3\rAfghanistan\rAFG\r2015\r23.600000\r4\rAlgeria\rDZA\r2000\r0.037746\r...\r...\r...\r...\r...\r503\rZambia\rZMB\r2015\r173.700000\r504\rZimbabwe\rZWE\r2000\r143.200000\r505\rZimbabwe\rZWE\r2005\r142.500000\r506\rZimbabwe\rZWE\r2010\r129.600000\r507\rZimbabwe\rZWE\r2015\r114.200000\r396 rows × 4 columns\nJoin the table with the Continents table df3_1 = df3.merge(dfc,left_on = 'Code', right_on = 'Three_Letter_Country_Code', how = 'left' )\rdf3_1\rEntity\rCode\rYear\rIncidence of malaria\rContinent_Name\rThree_Letter_Country_Code\r0\rAfghanistan\rAFG\r2000\r107.100000\rAsia\rAFG\r1\rAfghanistan\rAFG\r2005\r46.500000\rAsia\rAFG\r2\rAfghanistan\rAFG\r2010\r23.900000\rAsia\rAFG\r3\rAfghanistan\rAFG\r2015\r23.600000\rAsia\rAFG\r4\rAlgeria\rDZA\r2000\r0.037746\rAfrica\rDZA\r...\r...\r...\r...\r...\r...\r...\r391\rZambia\rZMB\r2015\r173.700000\rAfrica\rZMB\r392\rZimbabwe\rZWE\r2000\r143.200000\rAfrica\rZWE\r393\rZimbabwe\rZWE\r2005\r142.500000\rAfrica\rZWE\r394\rZimbabwe\rZWE\r2010\r129.600000\rAfrica\rZWE\r395\rZimbabwe\rZWE\r2015\r114.200000\rAfrica\rZWE\r396 rows × 6 columns\nPlot the trend of Mean Malaria Incidence for different Continents from 2000 to 2015 df3_2 = df3_1.groupby(by=[\u0026quot;Continent_Name\u0026quot;,\u0026quot;Year\u0026quot;]).agg('mean').reset_index()\rdf3_2\rContinent_Name\rYear\rIncidence of malaria\r0\rAfrica\r2000\r323.941229\r1\rAfrica\r2005\r267.446852\r2\rAfrica\r2010\r215.629814\r3\rAfrica\r2015\r173.338338\r4\rAsia\r2000\r58.626923\r5\rAsia\r2005\r42.831822\r6\rAsia\r2010\r22.113422\r7\rAsia\r2015\r6.034621\r8\rEurope\r2000\r590.066667\r9\rEurope\r2005\r102.033333\r10\rEurope\r2010\r0.166667\r11\rEurope\r2015\r0.000000\r12\rNorth America\r2000\r15.070000\r13\rNorth America\r2005\r11.170000\r14\rNorth America\r2010\r3.803387\r15\rNorth America\r2015\r1.960527\r16\rOceania\r2000\r293.666667\r17\rOceania\r2005\r277.666667\r18\rOceania\r2010\r132.633333\r19\rOceania\r2015\r64.166667\r20\rSouth America\r2000\r65.060000\r21\rSouth America\r2005\r50.880000\r22\rSouth America\r2010\r23.220000\r23\rSouth America\r2015\r15.620000\rfig, axes = plt.subplots(1, 1, figsize=(15,8))\rsns.lineplot(x='Year', y='Incidence of malaria',hue = 'Continent_Name' , data=df3_2)\rplt.title('Mean Malaria Incidence for different Continents from 2000 to 2015')\rplt.legend(loc='best', fontsize=10)\rpass\r","date":1601430668,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1601430668,"objectID":"d615eefdf8a45be7f5bbc20f81714da0","permalink":"https://ZhuoranHou.github.io/post/malaria_dataset_visualizations/","publishdate":"2020-09-29T21:51:08-04:00","relpermalink":"/post/malaria_dataset_visualizations/","section":"post","summary":"Visualizations of three Malaria datasets","tags":["Python","visualization"],"title":"Malaria datasets visualizations","type":"post"},{"authors":[],"categories":[],"content":"","date":1599228836,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1599228836,"objectID":"4bdbb4cce664c2f29b0b372e535dc80a","permalink":"https://ZhuoranHou.github.io/project/project_euler/","publishdate":"2020-09-04T10:13:56-04:00","relpermalink":"/project/project_euler/","section":"project","summary":"Three problems from the Project Euler","tags":[],"title":"Project Euler","type":"project"},{"authors":[],"categories":[],"content":"I solved these question using Python. Here are these questions and my solutions.\n1.Problem 5 Smallest multiple\n2520 is the smallest number that can be divided by each of the numbers from 1 to 10 without any remainder. What is the smallest positive number that is evenly divisible by all of the numbers from 1 to 20?\nSolved by 489287 (2020/09/03)\nExplanations \u0026amp; Answers:\nSince if this number can be evenly divisible by numbers from 11 to 20, it must can be evenly divisible by numbers from 11 to 20. So we only need to check 11-20.\nI wrote two loops with a inside loop checking if the remainder equals to zero and a outside loop checking numbers in the given range. ”div“ is the value indicating the status of divisibility of the number. If \u0026ldquo;div == 1\u0026rdquo; , then print the value.\ndef smallest_mul(n,a,b):\r\u0026quot;\u0026quot;\u0026quot;\rThis function calculate the smallest positive number that is evenly divisible by all of the numbers from given interval\r:param n: range of largest number tested.\r:param a: lower bound of the range of divisors.\r:param b: upper bound of the range of divisors.\routput: print the number that meet the criterion in the test range.\r\u0026quot;\u0026quot;\u0026quot;\rfor x in range(1,n):\rfor i in range(a,b+1):\rdiv = 0\rif x % i != 0:\rdiv = 1\rbreak\rif div == 0:\rprint(x)\rTwo examples:\nsmallest_mul(n=6000,a=1,b=10)\r2520\n5040\nsmallest_mul(n=300000000,a=11,b=20)\r232792560\nThus, the answer to is question is 232792560.\nProblem 34 Digit factorials\n145 is a curious number, as 1! + 4! + 5! = 1 + 24 + 120 = 145. Find the sum of all numbers which are equal to the sum of the factorial of their digits. Note: As 1! = 1 and 2! = 2 are not sums they are not included.\nSolved by 92955 (2020/09/03)\nExplanations \u0026amp; Answers:\nWe first try to find the searching range of \u0026ldquo;curious number\u0026rdquo;.\nimport math\rmath.factorial(9)*8\rmath.factorial(9)*7\r2903040\n2540160\nFrom the calculation above, we know that the numbers we search have at most 7 digits. For those have more digits, the sum of digit factorial will not be equal to the number itself.\nHere is the function that calculate the sum digit factorial of a given number. I first extracted each digit ((num // 10**(n-1)) % 10) and sum all the digit factorials.\ndef digsum(num):\r\u0026quot;\u0026quot;\u0026quot;\rThis function calculate the factorial sum of all digits of a number\r:param num: the input number\routput: return the sum of digit factorial\r\u0026quot;\u0026quot;\u0026quot;\rsumf = 0\rfor n in range(1,len(str(num))+1):\rsumf += math.factorial((num // 10**(n-1)) % 10)\rreturn sumf\rsumd = 0\rfor x in range(3,10000000):\rif x == digsum(x):\rprint(x)\rsumd += digsum(x)\rprint(f\u0026quot;The sum of all numbers which are equal to the sum of the factorial of their digits is {sumd}.\u0026quot;)\r145\n40585\nThe sum of all numbers which are equal to the sum of the factorial of their digits is 40730.\ndigsum(145)+digsum(40585)\r40730\nThus, the answer to this question is 40730.\nProblem 112 Bouncy numbers\nWorking from left-to-right if no digit is exceeded by the digit to its left it is called an increasing number; for example, 134468.\nSimilarly if no digit is exceeded by the digit to its right it is called a decreasing number; for example, 66420.\nWe shall call a positive integer that is neither increasing nor decreasing a \u0026ldquo;bouncy\u0026rdquo; number; for example, 155349.\nClearly there cannot be any bouncy numbers below one-hundred, but just over half of the numbers below one-thousand (525) are bouncy. In fact, the least number for which the proportion of bouncy numbers first reaches 50% is 538.\nSurprisingly, bouncy numbers become more and more common and by the time we reach 21780 the proportion of bouncy numbers is equal to 90%.\nFind the least number for which the proportion of bouncy numbers is exactly 99%.\nSolved by 23655 (2020/09/03)\nExplanations \u0026amp; Answers:\nFirst, I wrote a function to check if the number given is a bouncy number. If it is a bouncy number, then return \u0026ldquo;True\u0026rdquo;, otherwise \u0026ldquo;False\u0026rdquo;.\ndef check_b(num):\r\u0026quot;\u0026quot;\u0026quot;\rThis function is to check if the number is a bouncy number\r:param num: the input number\routput: return the boolean Values\r\u0026quot;\u0026quot;\u0026quot;\rnums = [0]*len(str(num))\rsta1 = True\rsta2 = True\rfor n in range(1,len(str(num))+1):\rnums[n-1] = (num // 10**(n-1)) % 10\rfor n in range(1,len(str(num))):\rif nums[n-1] \u0026lt; nums[n]:\rsta1 = False\rif nums[n-1] \u0026gt; nums[n]:\rsta2 = False\rreturn not(sta1 | sta2)\rThen, I wrote a while loop to find least number for which the proportion of bouncy numbers is exactly 99%.\nl = 0\rx = 1\rwhile True:\rif check_b(x):\rl += 1\rif l/x == 0.99:\rbreak\rx += 1\rprint(x)\r1587000\nThus, the answer to this question is 1587000.\n","date":1599227809,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1599227809,"objectID":"cea370f32292ec0829e73588b98497b2","permalink":"https://ZhuoranHou.github.io/post/project_euler/","publishdate":"2020-09-04T09:56:49-04:00","relpermalink":"/post/project_euler/","section":"post","summary":"Three problems from Project Euler and my solutions","tags":["Project Euler","Python"],"title":"Project Euler","type":"post"},{"authors":[],"categories":[],"content":"1.Pelican vs. Hugo These are two static site generators. On GitHub, Pelican has 9.8k stars while Hugo has 46.1k, indicating Hugo is much more popular (2020/08/20).Thus, I chose Hugo as my static site generator since it is much likely to have more technical support.\n2.Install Hugo After making the choice, I started to install the Hugo software on my windows 10 machine. I first downloaded hugo_extended_0.74.3_Windows-64bit.zip at Hugo releases and installed it according to the tutorials[2].\n3.Create a working directory Under the Sites (according to [2]) folder, I used Hugo to create a framework of the website by the code below:\n$ hugo new site personal_website\rAfter this, there was several files generated and these are the bases of the website.\n4.Choose a theme. After the previous step, I started to choose a theme for my personal page on Hugo Themes website. I chose the Academic theme, which provide many very sophisticated website templates and detailed tutorials[3]. I followed the tutorial and used the following commands to overwrite the files in the personal_website folder. You should first fork the original repository (\racademic-kickstart), clone your fork to the local PC and then initialize the theme by the command below (I had to say that I forgot to fork the repository first, and when I found it I almost finished the website):\n$ git clone https://github.com/username/academic-kickstart.git\r$ git submodule update --init --recursive\r5.Edit the framework and customize it This step took most of my time. The framework consists of several parts: home, publication, project and post. For the basic information of the website, I mainly edited files such as: config/_default/params.toml , config/_default/config.toml, config/_default/menus.toml , content/authors/admin/_index.md . These contribute to the biography part of the personal website. I did this mainly based on this tutorial [4]. The content part (publication, project and post) is mainly based on this tutorial [5].\n6.Deploy on site on GitHub During the local website construction process, I already uploaded my code to GitHub several times. It is always a good habit to commit the code to the repository once you make some small changes.\nI first created two empty repositories on my GitHub account:\na repository called personal_website : store the Hugo code\na repository called ZhuoranHou.github.io : store the website code generated by Hugo.\nThen, I committed and pushed my local code under personal_website to the GitHub repository personal_website. Afterwards, I changed the directory to the Sites folder, and typed the following code:\n$ git clone git@github.com:ZhuoranHou/ZhuoranHou.github.io.git\rThen changed the directory to the folder personal_website and typed this to deploy my website:\nhugo -d ../ZhuoranHou.github.io\rAfterwards, when I pushed the local ZhuoranHou.github.io folder to the GitHub repository, the personal website was successfully deployed.\nEvery time I made changes locally under the folder personal_website, I had to use the hugo -d command again to deploy the website and push it the GitHub repository.\nReferences\n1 https://www.tomasbeuzen.com/post/making-a-website-with-hugo/\n2 https://www.gohugo.org/doc/tutorials/installing-on-windows/\n3 https://wowchemy.com/docs/\n4 https://wowchemy.com/docs/get-started/\n5 https://wowchemy.com/docs/managing-content/\n","date":1597958921,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1597958921,"objectID":"ece7f1f4f1b0e0dc6e09d14250df80b9","permalink":"https://ZhuoranHou.github.io/post/how-to-build-personal-website/","publishdate":"2020-08-20T17:28:41-04:00","relpermalink":"/post/how-to-build-personal-website/","section":"post","summary":"Six steps to create an academic personal website","tags":["Hugo","GitHub"],"title":"How to build a personal website using Github and Hugo","type":"post"},{"authors":[],"categories":[],"content":"","date":1597957540,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1597957540,"objectID":"8787b862b182073c3d0d044eb6e1cbd4","permalink":"https://ZhuoranHou.github.io/project/personal/","publishdate":"2020-08-20T17:05:40-04:00","relpermalink":"/project/personal/","section":"project","summary":"Created a personal website using HUGO","tags":[],"title":"Personal website","type":"project"},{"authors":["Zhuoran Hou","Xiaofeng Wang","Yuchen Wang","Jiucun Wang","Judy Zhong"],"categories":[],"content":"","date":1580774400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1597944135,"objectID":"e2a484038dd6709506b84745ec69a4fd","permalink":"https://ZhuoranHou.github.io/publication/hou-2020-association/","publishdate":"2020-08-20T17:22:15.24283Z","relpermalink":"/publication/hou-2020-association/","section":"publication","summary":"Cognitive decline is a central feature in the aging process. Previous studies have indicated an association between depressive symptoms and cognitive decline in Caucasian populations. However, few studies have examined the effect of changes in depression on the trajectory of cognitive decline. Here, we included 580 participants with normal cognitive ability and complete cognitive and depression data from the Rugao Longevity and Ageing Study (RuLAS). We explored the relationship between depressive symptoms and cognitive decline in these participants.We examined how the change in depressive symptoms affected the trajectory in the HDS-R (the Revised Hasegawa Dementia Scale) scores by comparing cognition function in both the depression deterioration group and the depression steady group by using a linear mixed model. The results indicated that those with deteriorating depression tended to have faster cognitive declines than those with steady depression, indicated by the significance of the interaction term of GDS (Geriatric Depression Scale) groups and time (unadjusted model, β = − 0.673, p \u003c 0.001). The results remained significant after adding demographic covariates. Moreover, we found that those with the worst depressive symptoms at baseline had the worst cognition in subsequent years (GDS = 0 group vs. GDS ≥ group in the unadjusted model: β = − 1.522, p \u003c 0.003), while the slope of change was not significantly different among groups (GDS = 0 group × time vs. GDS \u003e =4 group × time in the unadjusted model: β = − 0.045, p = 0.857). Therefore, we found that depressive symptom deterioration was significantly associated with faster cognitive decline. Medical interventions for depression may decrease the number of older Chinese individuals who experience early-stage cognitive decline. ","tags":["Linear mixed model","Cognitive decline","Depression"],"title":"Association of depressive symptoms with decline of cognitive function—Rugao longevity and ageing study","type":"publication"},{"authors":["Zhuoran Hou","Jiucun Wang","Xiaofeng Wang"],"categories":[],"content":"","date":1556668800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1597944135,"objectID":"6a7d6bdb985c8ce170295ce540040c4b","permalink":"https://ZhuoranHou.github.io/publication/hou-2019-association/","publishdate":"2020-08-20T17:22:15.442298Z","relpermalink":"/publication/hou-2019-association/","section":"publication","summary":"Objective: In order to explore the relationship between depression symptoms and cognition impairment in the elderly in China. Methods: The data is from the Rugao Longevity and Ageing Study (RuLAS), including 1526 older people. We collected the demographic data and used Geriatric Depression Scale (GDS-15) to access the depression symptoms and Mini–Mental State Examination (MMSE) to evaluate the cognition in the participants. We used linear regression to analyze the relationship between GDS scores and MMSE score. Moreover, based on the grouping of education levels, we used logistic regression calculate the odds ratios for depression symptoms to cognition impairment. All the results were adjusted by adding demographic covariates. Results: In the crude model, we found that the GDS scores were significantly associated with MMSE scores (p\u003c0.001, β= -0.67), indicating that those with worse depression symptoms also had bad cognitive function. This result was still significant after adjusting covariates. Moreover, we further analyzed the data according to the levels of education. In the crude model, depression symptoms in older adults in the illiterate and primary school groups significantly increase the risk of cognition impairment (illiterate group: OR=2.22, 95% CI,1.45-3.48, p\u003c0.001; primary school group: OR=1.83, 95% CI, 1.08-3.07, p=0.024). The overall result was also significant ( OR=2.20，95%CI, 1.67-2.93, p\u003c0.001). These results did not change after adjusting covariates. All the results above showed that the depression symptoms in the Chinese older people significantly increase the risk of having cognition impairment. Conclusion: Depression symptoms in the elderly significantly increase the risk of cognition impairment in the elderly, and clinical interventions in depression symptoms in the elderly may help prevent and slow down the symptoms of cognitive decline in the elderly.","tags":["Depression","Cognition impairment"],"title":"Association study of depression symptoms and cognition impairment in Rugao Longevity and ageing study","type":"publication"},{"authors":["Zhuoran Hou","Jun Cao"],"categories":[],"content":"","date":1454630400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1597944136,"objectID":"2f1c32019e772b55ab4e50dfb5f9d372","permalink":"https://ZhuoranHou.github.io/publication/hou-2016-comparative/","publishdate":"2020-08-20T17:22:15.832712Z","relpermalink":"/publication/hou-2016-comparative/","section":"publication","summary":"P2X receptors are ligand-gated ion channels that can bind with the adenosine triphosphate (ATP) and have diverse functional roles in neuropathic pain, inflammation, special sense, and so on. In this study, 180 putative P2X genes, including 176 members in 32 animal species and 4 members in 3 species of lower plants, were identified. These genes were divided into 13 groups, including 7 groups in vertebrates and 6 groups in invertebrates and lower plants, through phylogenetic analysis. Their gene organization and motif composition are conserved in most predicted P2X members, while group-specific features were also found. Moreover, synteny relationships of the putative P2X genes in vertebrates are conserved while simultaneously experiencing a series of gene insertion, inversion, and transposition. Recombination signals were detected in almost all of the vertebrates and invertebrates, suggesting that intragenic recombination may play a significant role in the evolution of P2X genes. Selection analysis also identified some positively selected sites that acted on the evolution of most of the predicted P2X proteins. The phenomenon of alternative splicing occurred commonly in the putative P2X genes of vertebrates. This article explored in depth the evolutional relationship among different subtypes of P2X genes in animal and plants and might serve as a solid foundation for deciphering their functions in further studies. ","tags":["Evolution","Phylogenetic analysis"],"title":"Comparative study of the P2X gene family in animals and plants","type":"publication"}]